{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: will remove\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import typing as tp\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.metrics import MAP, Serendipity, MeanInvUserFreq, calc_metrics\n",
    "\n",
    "from rectools.models import BERT4RecModel, SASRecModel\n",
    "from rectools.models.nn.item_net import IdEmbeddingsItemNet\n",
    "from rectools.models.nn.transformer_base import TransformerModelBase\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !wget -q https://github.com/irsafilo/KION_DATASET/raw/f69775be31fa5779907cf0a92ddedb70037fb5ae/data_original.zip -O data_original.zip\n",
    "# !unzip -o data_original.zip\n",
    "# !rm data_original.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data_original\")\n",
    "\n",
    "interactions = (\n",
    "    pd.read_csv(DATA_PATH / 'interactions.csv', parse_dates=[\"last_watch_dt\"])\n",
    "    .rename(columns={\"last_watch_dt\": \"datetime\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions[Columns.Weight] = np.where(interactions['watched_pct'] > 10, 3, 1)\n",
    "\n",
    "# Split to train / test\n",
    "max_date = interactions[Columns.Datetime].max()\n",
    "train = interactions[interactions[Columns.Datetime] < max_date - pd.Timedelta(days=7)].copy()\n",
    "test = interactions[interactions[Columns.Datetime] >= max_date - pd.Timedelta(days=7)].copy()\n",
    "train.drop(train.query(\"total_dur < 300\").index, inplace=True)\n",
    "\n",
    "# drop items with less than 20 interactions in train\n",
    "items = train[\"item_id\"].value_counts()\n",
    "items = items[items >= 20]\n",
    "items = items.index.to_list()\n",
    "train = train[train[\"item_id\"].isin(items)]\n",
    "    \n",
    "# drop users with less than 2 interactions in train\n",
    "users = train[\"user_id\"].value_counts()\n",
    "users = users[users >= 2]\n",
    "users = users.index.to_list()\n",
    "train = train[(train[\"user_id\"].isin(users))]\n",
    "\n",
    "users = train[\"user_id\"].drop_duplicates().to_list()\n",
    "\n",
    "# drop cold users from test\n",
    "test_users_sasrec = test[Columns.User].unique()\n",
    "cold_users = set(test[Columns.User]) - set(train[Columns.User])\n",
    "test.drop(test[test[Columns.User].isin(cold_users)].index, inplace=True)\n",
    "test_users = test[Columns.User].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATA_PATH / 'items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process item features to the form of a flatten dataframe\n",
    "items = items.loc[items[Columns.Item].isin(train[Columns.Item])].copy()\n",
    "items[\"genre\"] = items[\"genres\"].str.lower().str.replace(\", \", \",\", regex=False).str.split(\",\")\n",
    "genre_feature = items[[\"item_id\", \"genre\"]].explode(\"genre\")\n",
    "genre_feature.columns = [\"id\", \"value\"]\n",
    "genre_feature[\"feature\"] = \"genre\"\n",
    "content_feature = items.reindex(columns=[Columns.Item, \"content_type\"])\n",
    "content_feature.columns = [\"id\", \"value\"]\n",
    "content_feature[\"feature\"] = \"content_type\"\n",
    "item_features = pd.concat((genre_feature, content_feature))\n",
    "\n",
    "candidate_items = interactions['item_id'].drop_duplicates().astype(int)\n",
    "test[\"user_id\"] = test[\"user_id\"].astype(int)\n",
    "test[\"item_id\"] = test[\"item_id\"].astype(int)\n",
    "\n",
    "catalog=train[Columns.Item].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_features = Dataset.construct(\n",
    "    interactions_df=train,\n",
    ")\n",
    "\n",
    "dataset_item_features = Dataset.construct(\n",
    "    interactions_df=train,\n",
    "    item_features_df=item_features,\n",
    "    cat_item_features=[\"genre\", \"content_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name = {\n",
    "    'MAP': MAP,\n",
    "    'MIUF': MeanInvUserFreq,\n",
    "    'Serendipity': Serendipity\n",
    "    \n",
    "\n",
    "}\n",
    "metrics = {}\n",
    "for metric_name, metric in metrics_name.items():\n",
    "    for k in (1, 5, 10):\n",
    "        metrics[f'{metric_name}@{k}'] = metric(k=k)\n",
    "\n",
    "# list with metrics results of all models\n",
    "features_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE=60\n",
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common model params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "TRAIN_MIN_USER_INTERACTIONS = 5\n",
    "SESSION_MAX_LEN = 50\n",
    "\n",
    "N_FACTORS = 256\n",
    "N_BLOCKS = 4\n",
    "N_HEADS = 4\n",
    "USE_POS_EMB = True\n",
    "LOSS = \"softmax\"\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2205.04507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from rectools.models.nn.bert4rec import BERT4RecDataPreparator\n",
    "from rectools.models.nn.constants import MASKING_VALUE\n",
    "from rectools.models.nn.transformer_lightning import TransformerLightningModule\n",
    "\n",
    "\n",
    "# \"MASK\" token is used for predicting one next token.\n",
    "class NextItemDataPreparator(BERT4RecDataPreparator):\n",
    "    \n",
    "    def _collate_fn_train(\n",
    "        self,\n",
    "        batch: List[Tuple[List[int], List[float]]],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Truncate each session from right to keep `session_max_len` items.\n",
    "        Do left padding until `session_max_len` is reached.\n",
    "        Split to `x`, `y`, and `yw`.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        x = np.zeros((batch_size, self.session_max_len))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        yw = np.zeros((batch_size, 1))\n",
    "        for i, (ses, ses_weights) in enumerate(batch):\n",
    "            session = ses.copy()\n",
    "            session[-1] = self.extra_token_ids[MASKING_VALUE]\n",
    "            x[i, -len(ses) :] = session  # ses: [session_len] -> x[i]: [session_max_len]\n",
    "            y[i] = ses[-1]  # ses: [session_len] -> y[i]: [1]\n",
    "            yw[i] = ses_weights[-1]  # ses_weights: [session_len] -> yw[i]: [1]\n",
    "\n",
    "        batch_dict = {\"x\": torch.LongTensor(x), \"y\": torch.LongTensor(y), \"yw\": torch.FloatTensor(yw)}\n",
    "        if self.n_negatives is not None:\n",
    "            negatives = torch.randint(\n",
    "                low=self.n_item_extra_tokens,\n",
    "                high=self.item_id_map.size,\n",
    "                size=(batch_size, 1, self.n_negatives),\n",
    "            )  # [batch_size, 1, n_negatives]\n",
    "            batch_dict[\"negatives\"] = negatives\n",
    "        return batch_dict\n",
    "\n",
    "\n",
    "# Last logits are used for reducing the number of calculations on training step.\n",
    "# You could also fill the y with zeros except for the last item in `_collate_fn_train`` and not change the training step \n",
    "class NextItemLightningModule(TransformerLightningModule):\n",
    "\n",
    "    def training_step(self, batch: tp.Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        x, y, w = batch[\"x\"], batch[\"y\"], batch[\"yw\"]\n",
    "        if self.loss == \"softmax\":\n",
    "            logits = self._get_full_catalog_logits(x)[:, -1: :]\n",
    "            loss = self._calc_softmax_loss(logits, y, w)\n",
    "        elif self.loss == \"BCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]\n",
    "            loss = self._calc_bce_loss(logits, y, w)\n",
    "        elif self.loss == \"gBCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]\n",
    "            loss = self._calc_gbce_loss(logits, y, w, negatives)\n",
    "        else:\n",
    "            loss = self._calc_custom_loss(batch, batch_idx)\n",
    "\n",
    "        self.log(self.train_loss_name, loss, on_step=False, on_epoch=True, prog_bar=self.verbose > 0)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "class NextItemSASRecModel(SASRecModel):\n",
    "\n",
    "    def _init_data_preparator(self) -> None:\n",
    "        self.data_preparator = self.data_preparator_type(\n",
    "            session_max_len=self.session_max_len,\n",
    "            n_negatives=self.n_negatives if self.loss != \"softmax\" else None,\n",
    "            batch_size=self.batch_size,\n",
    "            dataloader_num_workers=self.dataloader_num_workers,\n",
    "            train_min_user_interactions=self.train_min_user_interactions,\n",
    "            mask_prob=0.15,  # Add default `mask_proba`, because SASRec has not this parameter\n",
    "            get_val_mask_func=self.get_val_mask_func,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "bert4rec_nextitem_model = BERT4RecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=USE_POS_EMB,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=LOSS,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    use_causal_attn=False,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    ")\n",
    "\n",
    "sasrec_nextitem_model = NextItemSASRecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=USE_POS_EMB,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=LOSS,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.6 M \n",
      "---------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.478    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5300e10366694ff0935b1371a6330cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 9s, sys: 8.27 s, total: 7min 17s\n",
      "Wall time: 6min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.bert4rec.BERT4RecModel at 0x7f91fc1d1460>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bert4rec_nextitem_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 9min 37s, total: 11min 43s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recos = bert4rec_nextitem_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"bert_next_action_softmax\"\n",
    "features_results.append(metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 3.0 M \n",
      "---------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "12.176    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09910236725342dfa3e87ec8915c6102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 20s, sys: 8 s, total: 6min 28s\n",
      "Wall time: 5min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NextItemSASRecModel at 0x7f9202293760>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sasrec_nextitem_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 9min 35s, total: 11min 35s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recos = sasrec_nextitem_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"sasrec_next_action_softmax\"\n",
    "features_results.append(metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT4Rec with use_causal_attn = True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "bert4rec_nextitem_model_with_casual_mask = BERT4RecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=USE_POS_EMB,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_causal_attn=True,\n",
    "    loss=LOSS,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.6 M \n",
      "---------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.478    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542d3fbdd6db446381a5a7357fc5a174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 2s, sys: 6.52 s, total: 7min 9s\n",
      "Wall time: 6min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.bert4rec.BERT4RecModel at 0x7f908487a190>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bert4rec_nextitem_model_with_casual_mask.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 56s, sys: 9min 8s, total: 11min 5s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recos = bert4rec_nextitem_model_with_casual_mask.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"bert_next_action_softmax_causal\"\n",
    "features_results.append(metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import typing_extensions as tpe\n",
    "\n",
    "from rectools.dataset.dataset import Dataset, DatasetSchema\n",
    "from rectools.models.nn.transformer_data_preparator import TransformerDataPreparatorBase\n",
    "from rectools.models.nn.item_net import (\n",
    "    CatFeaturesItemNet,\n",
    "    IdEmbeddingsItemNet,\n",
    "    ItemNetBase,\n",
    "    ItemNetConstructorBase,\n",
    "    SumOfEmbeddingsConstructor,\n",
    ")\n",
    "from rectools.models import BERT4RecModel\n",
    "from rectools.models.nn.bert4rec import BERT4RecModelConfig, BERT4RecDataPreparator\n",
    "from rectools.models.nn.transformer_base import ValMaskCallable, TrainerCallable\n",
    "from rectools.models.nn.transformer_lightning import TransformerLightningModuleBase, TransformerLightningModule\n",
    "from rectools.models.nn.transformer_net_blocks import (\n",
    "    LearnableInversePositionalEncoding,\n",
    "    PreLNTransformerLayer,\n",
    "    PositionalEncodingBase,\n",
    "    TransformerLayersBase,\n",
    ")\n",
    "\n",
    "\n",
    "class AlBERT4RecSumOfEmbeddingsConstructor(SumOfEmbeddingsConstructor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_items: int,\n",
    "        emb_factors: int,\n",
    "        n_factors: int,\n",
    "        item_net_blocks: tp.Sequence[ItemNetBase],\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            n_items=n_items,\n",
    "            item_net_blocks=item_net_blocks\n",
    "        )\n",
    "        self.item_emb_proj = nn.Linear(emb_factors, n_factors)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: Dataset,\n",
    "        emb_factors: int,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset.item_id_map.size\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset(dataset, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, emb_factors, n_factors, item_net_blocks)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_schema(\n",
    "        cls,\n",
    "        dataset_schema: DatasetSchema,\n",
    "        emb_factors: int,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset_schema.items.n_hot\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset_schema(dataset_schema, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, emb_factors, n_factors, item_net_blocks)\n",
    "\n",
    "    def forward(self, items: torch.Tensor) -> torch.Tensor:\n",
    "        item_embs = super().forward(items)\n",
    "        item_embs = self.item_emb_proj(item_embs)\n",
    "        return item_embs\n",
    "\n",
    "\n",
    "class AlBERT4RecPreLNTransformerLayers(TransformerLayersBase):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks: int,\n",
    "        n_hidden_groups: int,\n",
    "        n_inner_groups: int,\n",
    "        n_factors: int,\n",
    "        n_heads: int,\n",
    "        dropout_rate: float,\n",
    "        ff_factors_multiplier: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        n_fitted_blocks = int(n_hidden_groups * n_inner_groups)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                PreLNTransformerLayer(\n",
    "                    # number of encoder layer (AlBERTLayers)\n",
    "                    # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L428\n",
    "                    n_factors,\n",
    "                    n_heads,\n",
    "                    dropout_rate,\n",
    "                    ff_factors_multiplier,\n",
    "                )\n",
    "                # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L469\n",
    "                for _ in range(n_fitted_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.n_layers_per_group = n_blocks / n_hidden_groups\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        seqs: torch.Tensor,\n",
    "        timeline_mask: torch.Tensor,\n",
    "        attn_mask: tp.Optional[torch.Tensor],\n",
    "        key_padding_mask: tp.Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        for block_idx in range(self.n_blocks):\n",
    "            group_idx = int(block_idx / self.n_layers_per_group)\n",
    "            for inner_layer_idx in range(self.n_inner_groups):\n",
    "                layer_idx = group_idx * self.n_inner_groups + inner_layer_idx\n",
    "                seqs = self.transformer_blocks[layer_idx](seqs, timeline_mask, attn_mask, key_padding_mask)\n",
    "        return seqs\n",
    "\n",
    "\n",
    "class AlBERT4RecModelConfig(BERT4RecModelConfig):\n",
    "\n",
    "    n_hidden_groups: int = 1\n",
    "    n_inner_groups: int = 1\n",
    "    emb_factors: int = 64\n",
    "\n",
    "\n",
    "class AlBERT4RecModel(BERT4RecModel):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/1909.11942\n",
    "    \"\"\"\n",
    "    \n",
    "    config_class = AlBERT4RecModelConfig\n",
    "\n",
    "    def __init__(  # pylint: disable=too-many-arguments, too-many-locals\n",
    "        self,\n",
    "        n_blocks: int = 2,\n",
    "        n_hidden_groups: int = 1,\n",
    "        n_inner_groups: int = 1,\n",
    "        n_heads: int = 4,\n",
    "        n_factors: int = 256,\n",
    "        emb_factors: int = 64,\n",
    "        dropout_rate: float = 0.0,\n",
    "        mask_prob: float = 0.15,\n",
    "        session_max_len: int = 100,\n",
    "        train_min_user_interactions: int = 2,\n",
    "        loss: str = \"softmax\",\n",
    "        n_negatives: int = 1,\n",
    "        gbce_t: float = 0.2,\n",
    "        lr: float = 0.001,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 3,\n",
    "        deterministic: bool = False,\n",
    "        verbose: int = 0,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        use_pos_emb: bool = True,\n",
    "        use_key_padding_mask: bool = True,\n",
    "        use_causal_attn: bool = False,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]] = (IdEmbeddingsItemNet, CatFeaturesItemNet ),\n",
    "        item_net_constructor_type: tp.Type[ItemNetConstructorBase] = AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "        pos_encoding_type: tp.Type[PositionalEncodingBase] = LearnableInversePositionalEncoding,\n",
    "        transformer_layers_type: tp.Type[TransformerLayersBase] = AlBERT4RecPreLNTransformerLayers,\n",
    "        data_preparator_type: tp.Type[TransformerDataPreparatorBase] = BERT4RecDataPreparator,\n",
    "        lightning_module_type: tp.Type[TransformerLightningModuleBase] = TransformerLightningModule,\n",
    "        get_val_mask_func: tp.Optional[ValMaskCallable] = None,\n",
    "        get_trainer_func: tp.Optional[TrainerCallable] = None,\n",
    "        recommend_batch_size: int = 256,\n",
    "        recommend_device: tp.Optional[str] = None,\n",
    "        recommend_n_threads: int = 0,\n",
    "        recommend_use_gpu_ranking: bool = True,  # TODO: remove after TorchRanker\n",
    "    ):\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        self.emb_factors = emb_factors\n",
    "\n",
    "        if n_blocks < n_hidden_groups:\n",
    "            warnings.warn(\n",
    "                \"When `n_hidden_groups` less than `n_blocks` that will use in the forward only one hidden group.\"\n",
    "            ) \n",
    "\n",
    "        super().__init__(\n",
    "            transformer_layers_type=transformer_layers_type,\n",
    "            data_preparator_type=data_preparator_type,\n",
    "            n_blocks=n_blocks,\n",
    "            n_heads=n_heads,\n",
    "            n_factors=n_factors,\n",
    "            use_pos_emb=use_pos_emb,\n",
    "            use_causal_attn=use_causal_attn,\n",
    "            use_key_padding_mask=use_key_padding_mask,\n",
    "            dropout_rate=dropout_rate,\n",
    "            session_max_len=session_max_len,\n",
    "            dataloader_num_workers=dataloader_num_workers,\n",
    "            batch_size=batch_size,\n",
    "            loss=loss,\n",
    "            n_negatives=n_negatives,\n",
    "            gbce_t=gbce_t,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            deterministic=deterministic,\n",
    "            recommend_device=recommend_device,\n",
    "            recommend_batch_size=recommend_batch_size,\n",
    "            recommend_n_threads=recommend_n_threads,\n",
    "            recommend_use_gpu_ranking=recommend_use_gpu_ranking,\n",
    "            train_min_user_interactions=train_min_user_interactions,\n",
    "            mask_prob=mask_prob,\n",
    "            item_net_block_types=item_net_block_types,\n",
    "            item_net_constructor_type=item_net_constructor_type,\n",
    "            pos_encoding_type=pos_encoding_type,\n",
    "            lightning_module_type=lightning_module_type,\n",
    "            get_val_mask_func=get_val_mask_func,\n",
    "            get_trainer_func=get_trainer_func,\n",
    "        )\n",
    "    \n",
    "    def _construct_item_net(self, dataset: Dataset) -> ItemNetConstructorBase:\n",
    "        return self.item_net_constructor_type.from_dataset(\n",
    "            dataset, self.emb_factors, self.n_factors, self.dropout_rate, self.item_net_block_types\n",
    "        )\n",
    "\n",
    "    def _construct_item_net_dataset_schema(self, dataset_schema: DatasetSchema) -> ItemNetConstructorBase:\n",
    "        return self.item_net_constructor_type.from_dataset_schema(\n",
    "            dataset_schema, self.emb_factors, self.n_factors, self.dropout_rate, self.item_net_block_types\n",
    "        )\n",
    "\n",
    "    def _init_transformer_layers(self) -> TransformerLayersBase:\n",
    "        return self.transformer_layers_type(\n",
    "            n_blocks=self.n_blocks,\n",
    "            n_hidden_groups=self.n_hidden_groups,\n",
    "            n_inner_groups=self.n_inner_groups,\n",
    "            n_factors=self.n_factors,\n",
    "            n_heads=self.n_heads,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_FACTORS = 64\n",
    "N_HIDDEN_GROUPS = 2\n",
    "N_INNER_GROUPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "albert_model = AlBERT4RecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    emb_factors=EMB_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_hidden_groups=N_HIDDEN_GROUPS,\n",
    "    n_inner_groups=N_INNER_GROUPS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=USE_POS_EMB,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    use_causal_attn=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=LOSS,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.0 M \n",
      "---------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.884     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6476efdc174f79a2209cac706b2f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 5s, sys: 6.01 s, total: 8min 11s\n",
      "Wall time: 7min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AlBERT4RecModel at 0x7f900c0f3eb0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "albert_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 10min 4s, total: 12min 12s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recos = albert_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"albert_softmax\"\n",
    "features_results.append(metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlSASRec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectools.models.nn.sasrec import SASRecModelConfig, SASRecDataPreparator, SASRecTransformerLayer\n",
    "\n",
    "\n",
    "class AlSASRecTransformerLayers(AlBERT4RecPreLNTransformerLayers):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks: int,\n",
    "        n_hidden_groups: int,\n",
    "        n_inner_groups: int,\n",
    "        n_factors: int,\n",
    "        n_heads: int,\n",
    "        dropout_rate: float,\n",
    "        ff_factors_multiplier: int = 4,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_blocks=n_blocks,\n",
    "            n_hidden_groups=n_hidden_groups,\n",
    "            n_inner_groups=n_inner_groups,\n",
    "            n_factors=n_factors,\n",
    "            n_heads=n_heads,\n",
    "            dropout_rate=dropout_rate,\n",
    "            ff_factors_multiplier=ff_factors_multiplier,\n",
    "        )\n",
    "        \n",
    "        n_fitted_blocks = int(n_hidden_groups * n_inner_groups)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                SASRecTransformerLayer(\n",
    "                    # number of encoder layer (AlBERTLayers)\n",
    "                    # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L428\n",
    "                    n_factors,\n",
    "                    n_heads,\n",
    "                    dropout_rate,\n",
    "                )\n",
    "                # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L469\n",
    "                for _ in range(n_fitted_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.last_layernorm = torch.nn.LayerNorm(n_factors, eps=1e-8)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        seqs: torch.Tensor,\n",
    "        timeline_mask: torch.Tensor,\n",
    "        attn_mask: tp.Optional[torch.Tensor],\n",
    "        key_padding_mask: tp.Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        seqs *= timeline_mask  # [batch_size, session_max_len, n_factors]\n",
    "        seqs = super().forward(seqs, timeline_mask, attn_mask, key_padding_mask)\n",
    "        seqs = self.last_layernorm(seqs)\n",
    "        return seqs\n",
    "\n",
    "\n",
    "class AlSASRecModelConfig(SASRecModelConfig):\n",
    "\n",
    "    n_hidden_groups: int = 1\n",
    "    n_inner_groups: int = 1\n",
    "    emb_factors: int = 64\n",
    "\n",
    "\n",
    "class AlSASRecModel(AlBERT4RecModel):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/1909.11942\n",
    "    \"\"\"\n",
    "    \n",
    "    config_class = AlSASRecModelConfig\n",
    "\n",
    "    def __init__(  # pylint: disable=too-many-arguments, too-many-locals\n",
    "        self,\n",
    "        n_blocks: int = 2,\n",
    "        n_hidden_groups: int = 1,\n",
    "        n_inner_groups: int = 1,\n",
    "        n_heads: int = 4,\n",
    "        n_factors: int = 256,\n",
    "        emb_factors: int = 64,\n",
    "        dropout_rate: float = 0.0,\n",
    "        mask_prob: float = 0.15,\n",
    "        session_max_len: int = 100,\n",
    "        train_min_user_interactions: int = 2,\n",
    "        loss: str = \"softmax\",\n",
    "        n_negatives: int = 1,\n",
    "        gbce_t: float = 0.2,\n",
    "        lr: float = 0.001,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 3,\n",
    "        deterministic: bool = False,\n",
    "        verbose: int = 0,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        use_pos_emb: bool = True,\n",
    "        use_key_padding_mask: bool = True,\n",
    "        use_causal_attn: bool = False,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]] = (IdEmbeddingsItemNet, CatFeaturesItemNet ),\n",
    "        item_net_constructor_type: tp.Type[ItemNetConstructorBase] = AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "        pos_encoding_type: tp.Type[PositionalEncodingBase] = LearnableInversePositionalEncoding,\n",
    "        transformer_layers_type: tp.Type[TransformerLayersBase] = AlSASRecTransformerLayers,  # Change\n",
    "        data_preparator_type: tp.Type[TransformerDataPreparatorBase] = SASRecDataPreparator,  # Change\n",
    "        lightning_module_type: tp.Type[TransformerLightningModuleBase] = TransformerLightningModule,\n",
    "        get_val_mask_func: tp.Optional[ValMaskCallable] = None,\n",
    "        get_trainer_func: tp.Optional[TrainerCallable] = None,\n",
    "        recommend_batch_size: int = 256,\n",
    "        recommend_device: tp.Optional[str] = None,\n",
    "        recommend_n_threads: int = 0,\n",
    "        recommend_use_gpu_ranking: bool = True,  # TODO: remove after TorchRanker\n",
    "    ):\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        self.emb_factors = emb_factors\n",
    "\n",
    "        if n_blocks < n_hidden_groups:\n",
    "            warnings.warn(\n",
    "                \"When `n_hidden_groups` less than `n_blocks` that will use in the forward only one hidden group.\"\n",
    "            ) \n",
    "\n",
    "        super().__init__(\n",
    "            transformer_layers_type=transformer_layers_type,\n",
    "            data_preparator_type=data_preparator_type,\n",
    "            n_blocks=n_blocks,\n",
    "            n_heads=n_heads,\n",
    "            n_factors=n_factors,\n",
    "            use_pos_emb=use_pos_emb,\n",
    "            use_causal_attn=use_causal_attn,\n",
    "            use_key_padding_mask=use_key_padding_mask,\n",
    "            dropout_rate=dropout_rate,\n",
    "            session_max_len=session_max_len,\n",
    "            dataloader_num_workers=dataloader_num_workers,\n",
    "            batch_size=batch_size,\n",
    "            loss=loss,\n",
    "            n_negatives=n_negatives,\n",
    "            gbce_t=gbce_t,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            deterministic=deterministic,\n",
    "            recommend_device=recommend_device,\n",
    "            recommend_batch_size=recommend_batch_size,\n",
    "            recommend_n_threads=recommend_n_threads,\n",
    "            recommend_use_gpu_ranking=recommend_use_gpu_ranking,\n",
    "            train_min_user_interactions=train_min_user_interactions,\n",
    "            mask_prob=mask_prob,\n",
    "            item_net_block_types=item_net_block_types,\n",
    "            item_net_constructor_type=item_net_constructor_type,\n",
    "            pos_encoding_type=pos_encoding_type,\n",
    "            lightning_module_type=lightning_module_type,\n",
    "            get_val_mask_func=get_val_mask_func,\n",
    "            get_trainer_func=get_trainer_func,\n",
    "        )\n",
    "        \n",
    "    def _init_data_preparator(self) -> None:\n",
    "        self.data_preparator = self.data_preparator_type(\n",
    "            session_max_len=self.session_max_len,\n",
    "            n_negatives=self.n_negatives if self.loss != \"softmax\" else None,\n",
    "            batch_size=self.batch_size,\n",
    "            dataloader_num_workers=self.dataloader_num_workers,\n",
    "            train_min_user_interactions=self.train_min_user_interactions,\n",
    "            get_val_mask_func=self.get_val_mask_func,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "alsasrec_model = AlSASRecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    emb_factors=EMB_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_hidden_groups=N_HIDDEN_GROUPS,\n",
    "    n_inner_groups=N_INNER_GROUPS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=USE_POS_EMB,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=LOSS,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    use_causal_attn=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "    transformer_layers_type=AlSASRecTransformerLayers,\n",
    "    data_preparator_type=SASRecDataPreparator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 787 K \n",
      "---------------------------------------------------------\n",
      "787 K     Trainable params\n",
      "0         Non-trainable params\n",
      "787 K     Total params\n",
      "3.150     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216f5ea472eb432d853a79e462e258ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 19s, sys: 7.47 s, total: 7min 26s\n",
      "Wall time: 7min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AlSASRecModel at 0x7f8fcaa35c10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "alsasrec_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 8min 28s, total: 10min 22s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recos = alsasrec_model.recommend(\n",
    "    users=test_users_sasrec,\n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"alsasrec_softmax\"\n",
    "features_results.append(metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAP@1</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>MIUF@1</th>\n",
       "      <th>MIUF@5</th>\n",
       "      <th>MIUF@10</th>\n",
       "      <th>Serendipity@1</th>\n",
       "      <th>Serendipity@5</th>\n",
       "      <th>Serendipity@10</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041575</td>\n",
       "      <td>0.073377</td>\n",
       "      <td>0.081693</td>\n",
       "      <td>3.770911</td>\n",
       "      <td>4.514560</td>\n",
       "      <td>5.049877</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>bert_next_action_softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.066949</td>\n",
       "      <td>0.072362</td>\n",
       "      <td>2.022693</td>\n",
       "      <td>2.805278</td>\n",
       "      <td>3.624555</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>sasrec_next_action_softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042906</td>\n",
       "      <td>0.073013</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>3.995624</td>\n",
       "      <td>4.609529</td>\n",
       "      <td>4.996234</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>bert_next_action_softmax_causal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038995</td>\n",
       "      <td>0.066442</td>\n",
       "      <td>0.074229</td>\n",
       "      <td>3.670593</td>\n",
       "      <td>4.394567</td>\n",
       "      <td>4.882518</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>albert_softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046672</td>\n",
       "      <td>0.079716</td>\n",
       "      <td>0.088329</td>\n",
       "      <td>3.798423</td>\n",
       "      <td>4.560263</td>\n",
       "      <td>5.155399</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>alsasrec_softmax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MAP@1     MAP@5    MAP@10    MIUF@1    MIUF@5   MIUF@10  Serendipity@1  \\\n",
       "0  0.041575  0.073377  0.081693  3.770911  4.514560  5.049877       0.000477   \n",
       "1  0.034768  0.066949  0.072362  2.022693  2.805278  3.624555       0.000008   \n",
       "2  0.042906  0.073013  0.081317  3.995624  4.609529  4.996234       0.000449   \n",
       "3  0.038995  0.066442  0.074229  3.670593  4.394567  4.882518       0.000292   \n",
       "4  0.046672  0.079716  0.088329  3.798423  4.560263  5.155399       0.000816   \n",
       "\n",
       "   Serendipity@5  Serendipity@10                            model  \n",
       "0       0.000460        0.000443         bert_next_action_softmax  \n",
       "1       0.000025        0.000029       sasrec_next_action_softmax  \n",
       "2       0.000381        0.000353  bert_next_action_softmax_causal  \n",
       "3       0.000275        0.000273                   albert_softmax  \n",
       "4       0.000745        0.000680                 alsasrec_softmax  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics = pd.DataFrame(features_results)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
