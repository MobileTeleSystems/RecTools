{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: will remove\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import typing as tp\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.metrics import MAP, Serendipity, MeanInvUserFreq, calc_metrics\n",
    "\n",
    "from rectools.models import BERT4RecModel, SASRecModel\n",
    "from rectools.models.nn.item_net import IdEmbeddingsItemNet\n",
    "from rectools.models.nn.transformer_base import TransformerModelBase\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !wget -q https://github.com/irsafilo/KION_DATASET/raw/f69775be31fa5779907cf0a92ddedb70037fb5ae/data_original.zip -O data_original.zip\n",
    "# !unzip -o data_original.zip\n",
    "# !rm data_original.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data_original\")\n",
    "\n",
    "interactions = (\n",
    "    pd.read_csv(DATA_PATH / 'interactions.csv', parse_dates=[\"last_watch_dt\"])\n",
    "    .rename(columns={\"last_watch_dt\": \"datetime\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions[Columns.Weight] = np.where(interactions['watched_pct'] > 10, 3, 1)\n",
    "\n",
    "# Split to train / test\n",
    "max_date = interactions[Columns.Datetime].max()\n",
    "train = interactions[interactions[Columns.Datetime] < max_date - pd.Timedelta(days=7)].copy()\n",
    "test = interactions[interactions[Columns.Datetime] >= max_date - pd.Timedelta(days=7)].copy()\n",
    "train.drop(train.query(\"total_dur < 300\").index, inplace=True)\n",
    "\n",
    "# drop items with less than 20 interactions in train\n",
    "items = train[\"item_id\"].value_counts()\n",
    "items = items[items >= 20]\n",
    "items = items.index.to_list()\n",
    "train = train[train[\"item_id\"].isin(items)]\n",
    "    \n",
    "# drop users with less than 2 interactions in train\n",
    "users = train[\"user_id\"].value_counts()\n",
    "users = users[users >= 2]\n",
    "users = users.index.to_list()\n",
    "train = train[(train[\"user_id\"].isin(users))]\n",
    "\n",
    "users = train[\"user_id\"].drop_duplicates().to_list()\n",
    "\n",
    "# drop cold users from test\n",
    "test_users_sasrec = test[Columns.User].unique()\n",
    "cold_users = set(test[Columns.User]) - set(train[Columns.User])\n",
    "test.drop(test[test[Columns.User].isin(cold_users)].index, inplace=True)\n",
    "test_users = test[Columns.User].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATA_PATH / 'items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process item features to the form of a flatten dataframe\n",
    "items = items.loc[items[Columns.Item].isin(train[Columns.Item])].copy()\n",
    "items[\"genre\"] = items[\"genres\"].str.lower().str.replace(\", \", \",\", regex=False).str.split(\",\")\n",
    "genre_feature = items[[\"item_id\", \"genre\"]].explode(\"genre\")\n",
    "genre_feature.columns = [\"id\", \"value\"]\n",
    "genre_feature[\"feature\"] = \"genre\"\n",
    "content_feature = items.reindex(columns=[Columns.Item, \"content_type\"])\n",
    "content_feature.columns = [\"id\", \"value\"]\n",
    "content_feature[\"feature\"] = \"content_type\"\n",
    "item_features = pd.concat((genre_feature, content_feature))\n",
    "\n",
    "candidate_items = interactions['item_id'].drop_duplicates().astype(int)\n",
    "test[\"user_id\"] = test[\"user_id\"].astype(int)\n",
    "test[\"item_id\"] = test[\"item_id\"].astype(int)\n",
    "\n",
    "catalog=train[Columns.Item].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_features = Dataset.construct(\n",
    "    interactions_df=train,\n",
    ")\n",
    "\n",
    "dataset_item_features = Dataset.construct(\n",
    "    interactions_df=train,\n",
    "    item_features_df=item_features,\n",
    "    cat_item_features=[\"genre\", \"content_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name = {\n",
    "    'MAP': MAP,\n",
    "    'MIUF': MeanInvUserFreq,\n",
    "    'Serendipity': Serendipity\n",
    "    \n",
    "\n",
    "}\n",
    "metrics = {}\n",
    "for metric_name, metric in metrics_name.items():\n",
    "    for k in (1, 5, 10):\n",
    "        metrics[f'{metric_name}@{k}'] = metric(k=k)\n",
    "\n",
    "# list with metrics results of all models\n",
    "features_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE=60\n",
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir(model: TransformerModelBase) -> Path:\n",
    "    \"\"\"\n",
    "    Get logging directory.\n",
    "    \"\"\"\n",
    "    path = model.fit_trainer.log_dir\n",
    "    return Path(path) / \"metrics.csv\"\n",
    "\n",
    "\n",
    "def get_losses(epoch_metrics_df: pd.DataFrame, is_val: bool) -> pd.DataFrame:\n",
    "    loss_df = epoch_metrics_df[[\"epoch\", \"train_loss\"]].dropna()\n",
    "    if is_val:\n",
    "        val_loss_df = epoch_metrics_df[[\"epoch\", \"val_loss\"]].dropna()\n",
    "        loss_df = pd.merge(loss_df, val_loss_df, how=\"inner\", on=\"epoch\")\n",
    "    return loss_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_val_metrics(epoch_metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    metrics_df = epoch_metrics_df.drop(columns=[\"train_loss\", \"val_loss\"]).dropna()\n",
    "    return metrics_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_log_values(model: TransformerModelBase, is_val: bool = False) -> tp.Tuple[pd.DataFrame, tp.Optional[pd.DataFrame]]:\n",
    "    log_path = get_log_dir(model)\n",
    "    epoch_metrics_df = pd.read_csv(log_path)\n",
    "\n",
    "    loss_df = get_losses(epoch_metrics_df, is_val)\n",
    "    val_metrics = None\n",
    "    if is_val:\n",
    "        val_metrics = get_val_metrics(epoch_metrics_df)\n",
    "    return loss_df, val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model common params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_EPOCHS = 5\n",
    "MAX_EPOCHS = 5\n",
    "TRAIN_MIN_USER_INTERACTIONS = 5\n",
    "SESSION_MAX_LEN = 50\n",
    "N_NEGATIVES = 5\n",
    "\n",
    "ACCELERATOR = \"gpu\"\n",
    "DEVICES = [0]\n",
    "\n",
    "N_FACTORS = 256\n",
    "N_BLOCKS = 4\n",
    "N_HEADS = 4\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2205.04507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from rectools.models.nn.transformer_data_preparator import TransformerDataPreparatorBase\n",
    "from rectools.models.nn.transformer_lightning import TransformerLightningModule\n",
    "\n",
    "\n",
    "# For Bert-like models need add MASK to `_collate_fn` methods.\n",
    "\n",
    "class NextItemDataPreparator(TransformerDataPreparatorBase):\n",
    "    \"\"\"Data preparator for SASRecModel.\"\"\"\n",
    "\n",
    "    train_session_max_len_addition: int = 1\n",
    "\n",
    "    def _collate_fn_train(\n",
    "        self,\n",
    "        batch: List[Tuple[List[int], List[float]]],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Truncate each session from right to keep `session_max_len` items.\n",
    "        Do left padding until `session_max_len` is reached.\n",
    "        Split to `x`, `y`, and `yw`.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        x = np.zeros((batch_size, self.session_max_len))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        yw = np.zeros((batch_size, 1))\n",
    "        for i, (ses, ses_weights) in enumerate(batch):\n",
    "            x[i, -len(ses) + 1 :] = ses[:-1]  # ses: [session_len] -> x[i]: [session_max_len]\n",
    "            y[i] = ses[-1]  # ses: [session_len] -> y[i]: [1]\n",
    "            yw[i] = ses_weights[-1]  # ses_weights: [session_len] -> yw[i]: [1]\n",
    "\n",
    "        batch_dict = {\"x\": torch.LongTensor(x), \"y\": torch.LongTensor(y), \"yw\": torch.FloatTensor(yw)}\n",
    "        if self.n_negatives is not None:\n",
    "            negatives = torch.randint(\n",
    "                low=self.n_item_extra_tokens,\n",
    "                high=self.item_id_map.size,\n",
    "                size=(batch_size, 1, self.n_negatives),\n",
    "            )  # [batch_size, 1, n_negatives]\n",
    "            batch_dict[\"negatives\"] = negatives\n",
    "        return batch_dict\n",
    "    \n",
    "    def _collate_fn_recommend(self, batch: List[Tuple[List[int], List[float]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Right truncation, left padding to session_max_len\"\"\"\n",
    "        x = np.zeros((len(batch), self.session_max_len))\n",
    "        for i, (ses, _) in enumerate(batch):\n",
    "            x[i, -len(ses) :] = ses[-self.session_max_len :]\n",
    "        return {\"x\": torch.LongTensor(x)}\n",
    "\n",
    "\n",
    "class NextItemLightningModule(TransformerLightningModule):\n",
    "\n",
    "    def training_step(self, batch: tp.Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        x, y, w = batch[\"x\"], batch[\"y\"], batch[\"yw\"]\n",
    "        if self.loss == \"softmax\":\n",
    "            logits = self._get_full_catalog_logits(x)[:, -1: :]\n",
    "            loss = self._calc_softmax_loss(logits, y, w)\n",
    "        elif self.loss == \"BCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]\n",
    "            loss = self._calc_bce_loss(logits, y, w)\n",
    "        elif self.loss == \"gBCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]\n",
    "            loss = self._calc_gbce_loss(logits, y, w, negatives)\n",
    "        else:\n",
    "            loss = self._calc_custom_loss(batch, batch_idx)\n",
    "\n",
    "        self.log(self.train_loss_name, loss, on_step=False, on_epoch=True, prog_bar=self.verbose > 0)\n",
    "\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nextitem_trainer():\n",
    "    return Trainer(\n",
    "        accelerator=ACCELERATOR,\n",
    "        devices=DEVICES,\n",
    "        min_epochs=MIN_EPOCHS,\n",
    "        max_epochs=MAX_EPOCHS, \n",
    "        deterministic=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "sasrec_nextitem_model = SASRecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=\"softmax\",\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    "    get_trainer_func=get_nextitem_trainer,\n",
    ")\n",
    "\n",
    "N_NEGATIVES = 5\n",
    "\n",
    "sasrec_nextitem_bce_model = SASRecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=\"BCE\",\n",
    "    n_negatives=N_NEGATIVES,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    "    get_trainer_func=get_nextitem_trainer,\n",
    ")\n",
    "\n",
    "sasrec_nextitem_gbce_model = SASRecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=\"gBCE\",\n",
    "    n_negatives=N_NEGATIVES,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    "    get_trainer_func=get_nextitem_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 3.0 M \n",
      "---------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "12.175    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416cb79cab3d4a94a770d90f737b8447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 21s, sys: 6.17 s, total: 4min 27s\n",
      "Wall time: 4min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x7fc82bf640d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sasrec_nextitem_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 8min 36s, total: 10min 26s\n",
      "Wall time: 20.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = sasrec_nextitem_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"sasrec_next_action_softmax\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 3.0 M \n",
      "---------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "12.175    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0322ce76b9a431aa60704e9752839b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 22s, sys: 6.2 s, total: 4min 28s\n",
      "Wall time: 4min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x7fc9fd8f9af0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sasrec_nextitem_bce_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 11min 4s, total: 13min 20s\n",
      "Wall time: 23.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'},\n",
       " {'MAP@1': 0.030642412703662827,\n",
       "  'MAP@5': 0.06354385798198152,\n",
       "  'MAP@10': 0.06939375803777235,\n",
       "  'MIUF@1': 2.1760997525872234,\n",
       "  'MIUF@5': 2.8881699041484103,\n",
       "  'MIUF@10': 3.6832272807506397,\n",
       "  'Serendipity@1': 1.0021156220458501e-05,\n",
       "  'Serendipity@5': 2.479979331575515e-05,\n",
       "  'Serendipity@10': 3.303582017928996e-05,\n",
       "  'model': 'sasrec_next_action_bce'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = sasrec_nextitem_bce_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"sasrec_next_action_bce\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 3.0 M \n",
      "---------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "12.175    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01a95dc2ec54d2cab4dc33a3b116759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 34s, sys: 5.86 s, total: 4min 39s\n",
      "Wall time: 4min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x7fc82bf64a60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sasrec_nextitem_gbce_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 16s, sys: 10min 4s, total: 12min 21s\n",
      "Wall time: 23.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'},\n",
       " {'MAP@1': 0.030642412703662827,\n",
       "  'MAP@5': 0.06354385798198152,\n",
       "  'MAP@10': 0.06939375803777235,\n",
       "  'MIUF@1': 2.1760997525872234,\n",
       "  'MIUF@5': 2.8881699041484103,\n",
       "  'MIUF@10': 3.6832272807506397,\n",
       "  'Serendipity@1': 1.0021156220458501e-05,\n",
       "  'Serendipity@5': 2.479979331575515e-05,\n",
       "  'Serendipity@10': 3.303582017928996e-05,\n",
       "  'model': 'sasrec_next_action_bce'},\n",
       " {'MAP@1': 0.03477402284752523,\n",
       "  'MAP@5': 0.06698163027041105,\n",
       "  'MAP@10': 0.07277966423896895,\n",
       "  'MIUF@1': 2.020424706174193,\n",
       "  'MIUF@5': 2.823113684972394,\n",
       "  'MIUF@10': 3.7625225192486647,\n",
       "  'Serendipity@1': 7.626538834693788e-06,\n",
       "  'Serendipity@5': 2.4311904032477694e-05,\n",
       "  'Serendipity@10': 3.577375322457377e-05,\n",
       "  'model': 'sasrec_next_action_gbce'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = sasrec_nextitem_gbce_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"sasrec_next_action_gbce\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>loss_type</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>loss_type</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>loss_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18.603785</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767060</td>\n",
       "      <td>bce</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.375935</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721544</td>\n",
       "      <td>bce</td>\n",
       "      <td>1</td>\n",
       "      <td>0.627388</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18.355379</td>\n",
       "      <td>softmax</td>\n",
       "      <td>2</td>\n",
       "      <td>0.715669</td>\n",
       "      <td>bce</td>\n",
       "      <td>2</td>\n",
       "      <td>0.622248</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18.337034</td>\n",
       "      <td>softmax</td>\n",
       "      <td>3</td>\n",
       "      <td>0.711058</td>\n",
       "      <td>bce</td>\n",
       "      <td>3</td>\n",
       "      <td>0.620583</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18.291851</td>\n",
       "      <td>softmax</td>\n",
       "      <td>4</td>\n",
       "      <td>0.709537</td>\n",
       "      <td>bce</td>\n",
       "      <td>4</td>\n",
       "      <td>0.617849</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss loss_type  epoch  train_loss loss_type  epoch  \\\n",
       "0      0   18.603785   softmax      0    0.767060       bce      0   \n",
       "1      1   18.375935   softmax      1    0.721544       bce      1   \n",
       "2      2   18.355379   softmax      2    0.715669       bce      2   \n",
       "3      3   18.337034   softmax      3    0.711058       bce      3   \n",
       "4      4   18.291851   softmax      4    0.709537       bce      4   \n",
       "\n",
       "   train_loss loss_type  \n",
       "0    0.670500      gbce  \n",
       "1    0.627388      gbce  \n",
       "2    0.622248      gbce  \n",
       "3    0.620583      gbce  \n",
       "4    0.617849      gbce  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_loss_df, _ = get_log_values(sasrec_nextitem_model, is_val=False)\n",
    "softmax_loss_df[\"loss_type\"] = \"softmax\"\n",
    "bce_loss_df, _ = get_log_values(sasrec_nextitem_bce_model, is_val=False)\n",
    "bce_loss_df[\"loss_type\"] = \"bce\"\n",
    "gbce_loss_df, _ = get_log_values(sasrec_nextitem_gbce_model, is_val=False)\n",
    "gbce_loss_df[\"loss_type\"] = \"gbce\"\n",
    "pd.concat([softmax_loss_df, bce_loss_df, gbce_loss_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use_causal_attn=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "nextitem_model_with_casual_mask = SASRecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_causal_attn=True,\n",
    "    loss=\"softmax\",\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=NextItemDataPreparator,\n",
    "    lightning_module_type=NextItemLightningModule,\n",
    "    get_trainer_func=get_nextitem_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 3.0 M \n",
      "---------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "12.175    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "face29a6ba7440b185a4258d34e1632b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 24s, sys: 5.98 s, total: 4min 30s\n",
      "Wall time: 4min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x7fc743ca8040>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nextitem_model_with_casual_mask.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 34s, sys: 10min 11s, total: 12min 46s\n",
      "Wall time: 22.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'},\n",
       " {'MAP@1': 0.030642412703662827,\n",
       "  'MAP@5': 0.06354385798198152,\n",
       "  'MAP@10': 0.06939375803777235,\n",
       "  'MIUF@1': 2.1760997525872234,\n",
       "  'MIUF@5': 2.8881699041484103,\n",
       "  'MIUF@10': 3.6832272807506397,\n",
       "  'Serendipity@1': 1.0021156220458501e-05,\n",
       "  'Serendipity@5': 2.479979331575515e-05,\n",
       "  'Serendipity@10': 3.303582017928996e-05,\n",
       "  'model': 'sasrec_next_action_bce'},\n",
       " {'MAP@1': 0.03477402284752523,\n",
       "  'MAP@5': 0.06698163027041105,\n",
       "  'MAP@10': 0.07277966423896895,\n",
       "  'MIUF@1': 2.020424706174193,\n",
       "  'MIUF@5': 2.823113684972394,\n",
       "  'MIUF@10': 3.7625225192486647,\n",
       "  'Serendipity@1': 7.626538834693788e-06,\n",
       "  'Serendipity@5': 2.4311904032477694e-05,\n",
       "  'Serendipity@10': 3.577375322457377e-05,\n",
       "  'model': 'sasrec_next_action_gbce'},\n",
       " {'MAP@1': 0.03476408463323054,\n",
       "  'MAP@5': 0.06706105865754652,\n",
       "  'MAP@10': 0.07246719992626127,\n",
       "  'MIUF@1': 2.029366296672227,\n",
       "  'MIUF@5': 2.814538100018135,\n",
       "  'MIUF@10': 3.616757947914895,\n",
       "  'Serendipity@1': 8.864940805646943e-06,\n",
       "  'Serendipity@5': 2.7204123658154736e-05,\n",
       "  'Serendipity@10': 2.9214145318701434e-05,\n",
       "  'model': 'sasrec_next_action_softmax_casual'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = nextitem_model_with_casual_mask.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"sasrec_next_action_softmax_casual\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18.612335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.424408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18.361567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18.333748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18.335825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss\n",
       "0      0   18.612335\n",
       "1      1   18.424408\n",
       "2      2   18.361567\n",
       "3      3   18.333748\n",
       "4      4   18.335825"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df, _ = get_log_values(nextitem_model_with_casual_mask, is_val=False)\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import typing_extensions as tpe\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from rectools.dataset.dataset import Dataset, DatasetSchema\n",
    "from rectools.models.nn.transformer_data_preparator import TransformerDataPreparatorBase\n",
    "from rectools.models.nn.item_net import (\n",
    "    CatFeaturesItemNet,\n",
    "    IdEmbeddingsItemNet,\n",
    "    ItemNetBase,\n",
    "    ItemNetConstructorBase,\n",
    "    SumOfEmbeddingsConstructor,\n",
    ")\n",
    "from rectools.models import BERT4RecModel\n",
    "from rectools.models.nn.bert4rec import BERT4RecModelConfig, BERT4RecDataPreparator\n",
    "from rectools.models.nn.transformer_base import ValMaskCallable, TrainerCallable\n",
    "from rectools.models.nn.transformer_lightning import TransformerLightningModuleBase, TransformerLightningModule\n",
    "from rectools.models.nn.transformer_net_blocks import (\n",
    "    LearnableInversePositionalEncoding,\n",
    "    PreLNTransformerLayer,\n",
    "    PositionalEncodingBase,\n",
    "    TransformerLayersBase,\n",
    ")\n",
    "\n",
    "\n",
    "class AlBERT4RecSumOfEmbeddingsConstructor(SumOfEmbeddingsConstructor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_items: int,\n",
    "        emb_factors: int,\n",
    "        n_factors: int,\n",
    "        item_net_blocks: tp.Sequence[ItemNetBase],\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            n_items=n_items,\n",
    "            item_net_blocks=item_net_blocks\n",
    "        )\n",
    "        self.item_emb_proj = nn.Linear(emb_factors, n_factors)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: Dataset,\n",
    "        emb_factors: int,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset.item_id_map.size\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset(dataset, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, emb_factors, n_factors, item_net_blocks)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_schema(\n",
    "        cls,\n",
    "        dataset_schema: DatasetSchema,\n",
    "        emb_factors: int,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset_schema.items.n_hot\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset_schema(dataset_schema, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, emb_factors, n_factors, item_net_blocks)\n",
    "\n",
    "    def forward(self, items: torch.Tensor) -> torch.Tensor:\n",
    "        item_embs = super().forward(items)\n",
    "        item_embs = self.item_emb_proj(item_embs)\n",
    "        return item_embs\n",
    "\n",
    "\n",
    "class AlBERT4RecPreLNTransformerLayers(TransformerLayersBase):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks: int,\n",
    "        n_hidden_groups: int,\n",
    "        n_inner_groups: int,\n",
    "        n_factors: int,\n",
    "        n_heads: int,\n",
    "        dropout_rate: float,\n",
    "        ff_factors_multiplier: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        n_fitted_blocks = int(n_hidden_groups * n_inner_groups)\n",
    "        self.transformer_layers = nn.ModuleList(\n",
    "            [\n",
    "                PreLNTransformerLayer(\n",
    "                    # number of encoder layer (AlBERTLayers)\n",
    "                    # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L428\n",
    "                    n_factors,\n",
    "                    n_heads,\n",
    "                    dropout_rate,\n",
    "                    ff_factors_multiplier,\n",
    "                ) \n",
    "                # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L469\n",
    "                for _ in range(n_fitted_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.n_layers_per_group = n_blocks / n_hidden_groups\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        seqs: torch.Tensor,\n",
    "        timeline_mask: torch.Tensor,\n",
    "        attn_mask: tp.Optional[torch.Tensor],\n",
    "        key_padding_mask: tp.Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        for layer_idx_in_group in range(self.n_blocks):\n",
    "            group_idx = int(layer_idx_in_group / self.n_layers_per_group)\n",
    "            seqs = self.transformer_layers[group_idx](seqs, timeline_mask, attn_mask, key_padding_mask)\n",
    "        return seqs\n",
    "\n",
    "\n",
    "class AlBERT4RecModelConfig(BERT4RecModelConfig):\n",
    "\n",
    "    n_hidden_groups: int = 1\n",
    "    n_inner_groups: int = 1\n",
    "    emb_factors: int = 64\n",
    "\n",
    "\n",
    "class AlBERT4RecModel(BERT4RecModel):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/1909.11942\n",
    "    \"\"\"\n",
    "    \n",
    "    config_class = AlBERT4RecModelConfig\n",
    "\n",
    "    def __init__(  # pylint: disable=too-many-arguments, too-many-locals\n",
    "        self,\n",
    "        n_blocks: int = 2,\n",
    "        n_hidden_groups: int = 1,\n",
    "        n_inner_groups: int = 1,\n",
    "        n_heads: int = 4,\n",
    "        n_factors: int = 256,\n",
    "        emb_factors: int = 64,\n",
    "        dropout_rate: float = 0.0,\n",
    "        mask_prob: float = 0.15,\n",
    "        session_max_len: int = 100,\n",
    "        train_min_user_interactions: int = 2,\n",
    "        loss: str = \"softmax\",\n",
    "        n_negatives: int = 1,\n",
    "        gbce_t: float = 0.2,\n",
    "        lr: float = 0.001,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 3,\n",
    "        deterministic: bool = False,\n",
    "        verbose: int = 0,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        use_pos_emb: bool = True,\n",
    "        use_key_padding_mask: bool = True,\n",
    "        use_causal_attn: bool = False,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]] = (IdEmbeddingsItemNet, CatFeaturesItemNet ),\n",
    "        item_net_constructor_type: tp.Type[ItemNetConstructorBase] = AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "        pos_encoding_type: tp.Type[PositionalEncodingBase] = LearnableInversePositionalEncoding,\n",
    "        transformer_layers_type: tp.Type[TransformerLayersBase] = AlBERT4RecPreLNTransformerLayers,\n",
    "        data_preparator_type: tp.Type[TransformerDataPreparatorBase] = BERT4RecDataPreparator,\n",
    "        lightning_module_type: tp.Type[TransformerLightningModuleBase] = TransformerLightningModule,\n",
    "        get_val_mask_func: tp.Optional[ValMaskCallable] = None,\n",
    "        get_trainer_func: tp.Optional[TrainerCallable] = None,\n",
    "        recommend_batch_size: int = 256,\n",
    "        recommend_device: tp.Optional[str] = None,\n",
    "        recommend_n_threads: int = 0,\n",
    "        recommend_use_gpu_ranking: bool = True,  # TODO: remove after TorchRanker\n",
    "    ):\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        self.emb_factors = emb_factors\n",
    "\n",
    "        if n_blocks < n_hidden_groups:\n",
    "            warnings.warn(\n",
    "                \"When `n_hidden_groups` less than `n_blocks` that will use in the forward only one hidden group.\"\n",
    "            ) \n",
    "\n",
    "        super().__init__(\n",
    "            transformer_layers_type=transformer_layers_type,\n",
    "            data_preparator_type=data_preparator_type,\n",
    "            n_blocks=n_blocks,\n",
    "            n_heads=n_heads,\n",
    "            n_factors=n_factors,\n",
    "            use_pos_emb=use_pos_emb,\n",
    "            use_causal_attn=use_causal_attn,\n",
    "            use_key_padding_mask=use_key_padding_mask,\n",
    "            dropout_rate=dropout_rate,\n",
    "            session_max_len=session_max_len,\n",
    "            dataloader_num_workers=dataloader_num_workers,\n",
    "            batch_size=batch_size,\n",
    "            loss=loss,\n",
    "            n_negatives=n_negatives,\n",
    "            gbce_t=gbce_t,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            deterministic=deterministic,\n",
    "            recommend_device=recommend_device,\n",
    "            recommend_batch_size=recommend_batch_size,\n",
    "            recommend_n_threads=recommend_n_threads,\n",
    "            recommend_use_gpu_ranking=recommend_use_gpu_ranking,\n",
    "            train_min_user_interactions=train_min_user_interactions,\n",
    "            mask_prob=mask_prob,\n",
    "            item_net_block_types=item_net_block_types,\n",
    "            item_net_constructor_type=item_net_constructor_type,\n",
    "            pos_encoding_type=pos_encoding_type,\n",
    "            lightning_module_type=lightning_module_type,\n",
    "            get_val_mask_func=get_val_mask_func,\n",
    "            get_trainer_func=get_trainer_func,\n",
    "        )\n",
    "    \n",
    "    def init_item_net_from_dataset(self, dataset: Dataset) -> ItemNetConstructorBase:\n",
    "        return self.item_net_constructor_type.from_dataset(\n",
    "            dataset, self.emb_factors, self.n_factors, self.dropout_rate, self.item_net_block_types\n",
    "        )\n",
    "\n",
    "    def init_item_net_from_dataset_schema(self, dataset_schema: DatasetSchema) -> ItemNetConstructorBase:\n",
    "        return self.item_net_constructor_type.from_dataset_schema(\n",
    "            dataset_schema, self.emb_factors, self.n_factors, self.dropout_rate, self.item_net_block_types\n",
    "        )\n",
    "\n",
    "    def init_transformer_layers(self) -> TransformerLayersBase:\n",
    "        return self.transformer_layers_type(\n",
    "            n_blocks=self.n_blocks,\n",
    "            n_hidden_groups=self.n_hidden_groups,\n",
    "            n_inner_groups=self.n_inner_groups,\n",
    "            n_factors=self.n_factors,\n",
    "            n_heads=self.n_heads,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albert_trainer():\n",
    "    return Trainer(\n",
    "        accelerator=ACCELERATOR,\n",
    "        devices=DEVICES,\n",
    "        min_epochs=MIN_EPOCHS,\n",
    "        max_epochs=MAX_EPOCHS, \n",
    "        deterministic=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_FACTORS = 64\n",
    "N_HIDDEN_GROUPS = 2\n",
    "N_INNER_GROUPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "albert_model = AlBERT4RecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    emb_factors=EMB_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_hidden_groups=N_HIDDEN_GROUPS,\n",
    "    n_inner_groups=N_INNER_GROUPS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    use_causal_attn=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=\"softmax\",\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,\n",
    "    get_trainer_func=get_albert_trainer,\n",
    ")\n",
    "\n",
    "\n",
    "N_NEGATIVES = 5\n",
    "\n",
    "albert_model_bce = AlBERT4RecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    emb_factors=EMB_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_hidden_groups=N_HIDDEN_GROUPS,\n",
    "    n_inner_groups=N_INNER_GROUPS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    use_causal_attn=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=\"BCE\",\n",
    "    n_negatives=N_NEGATIVES,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,\n",
    "    get_trainer_func=get_albert_trainer,\n",
    ")\n",
    "\n",
    "albert_model_gbce = AlBERT4RecModel(\n",
    "    n_factors=N_FACTORS,\n",
    "    emb_factors=EMB_FACTORS,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_hidden_groups=N_HIDDEN_GROUPS,\n",
    "    n_inner_groups=N_INNER_GROUPS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interactions=TRAIN_MIN_USER_INTERACTIONS,\n",
    "    session_max_len=SESSION_MAX_LEN,\n",
    "    lr=LR,\n",
    "    use_causal_attn=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss=\"gBCE\",\n",
    "    n_negatives=N_NEGATIVES,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,\n",
    "    get_trainer_func=get_albert_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.0 M \n",
      "---------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.884     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509033392fc24fcda40a3f7275e79907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 57s, sys: 4.42 s, total: 6min 1s\n",
      "Wall time: 5min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AlBERT4RecModel at 0x7fc7205daeb0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "albert_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 33s, sys: 10min 25s, total: 12min 59s\n",
      "Wall time: 25.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'},\n",
       " {'MAP@1': 0.030642412703662827,\n",
       "  'MAP@5': 0.06354385798198152,\n",
       "  'MAP@10': 0.06939375803777235,\n",
       "  'MIUF@1': 2.1760997525872234,\n",
       "  'MIUF@5': 2.8881699041484103,\n",
       "  'MIUF@10': 3.6832272807506397,\n",
       "  'Serendipity@1': 1.0021156220458501e-05,\n",
       "  'Serendipity@5': 2.479979331575515e-05,\n",
       "  'Serendipity@10': 3.303582017928996e-05,\n",
       "  'model': 'sasrec_next_action_bce'},\n",
       " {'MAP@1': 0.03477402284752523,\n",
       "  'MAP@5': 0.06698163027041105,\n",
       "  'MAP@10': 0.07277966423896895,\n",
       "  'MIUF@1': 2.020424706174193,\n",
       "  'MIUF@5': 2.823113684972394,\n",
       "  'MIUF@10': 3.7625225192486647,\n",
       "  'Serendipity@1': 7.626538834693788e-06,\n",
       "  'Serendipity@5': 2.4311904032477694e-05,\n",
       "  'Serendipity@10': 3.577375322457377e-05,\n",
       "  'model': 'sasrec_next_action_gbce'},\n",
       " {'MAP@1': 0.03476408463323054,\n",
       "  'MAP@5': 0.06706105865754652,\n",
       "  'MAP@10': 0.07246719992626127,\n",
       "  'MIUF@1': 2.029366296672227,\n",
       "  'MIUF@5': 2.814538100018135,\n",
       "  'MIUF@10': 3.616757947914895,\n",
       "  'Serendipity@1': 8.864940805646943e-06,\n",
       "  'Serendipity@5': 2.7204123658154736e-05,\n",
       "  'Serendipity@10': 2.9214145318701434e-05,\n",
       "  'model': 'sasrec_next_action_softmax_casual'},\n",
       " {'MAP@1': 0.04001981445941568,\n",
       "  'MAP@5': 0.06678375449816408,\n",
       "  'MAP@10': 0.07485368631340723,\n",
       "  'MIUF@1': 4.2954472973798925,\n",
       "  'MIUF@5': 5.091198216891677,\n",
       "  'MIUF@10': 5.485630716990267,\n",
       "  'Serendipity@1': 0.0005120932240612023,\n",
       "  'Serendipity@5': 0.00044805495381661963,\n",
       "  'Serendipity@10': 0.000418576504004872,\n",
       "  'model': 'albert_softmax'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = albert_model.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"albert_softmax\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.0 M \n",
      "---------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.884     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ca6e45108f4bf28b21235279ade93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 59s, sys: 3.91 s, total: 5min 3s\n",
      "Wall time: 4min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AlBERT4RecModel at 0x7fc7205da4f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "albert_model_bce.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 10min 19s, total: 13min 6s\n",
      "Wall time: 22.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'},\n",
       " {'MAP@1': 0.030642412703662827,\n",
       "  'MAP@5': 0.06354385798198152,\n",
       "  'MAP@10': 0.06939375803777235,\n",
       "  'MIUF@1': 2.1760997525872234,\n",
       "  'MIUF@5': 2.8881699041484103,\n",
       "  'MIUF@10': 3.6832272807506397,\n",
       "  'Serendipity@1': 1.0021156220458501e-05,\n",
       "  'Serendipity@5': 2.479979331575515e-05,\n",
       "  'Serendipity@10': 3.303582017928996e-05,\n",
       "  'model': 'sasrec_next_action_bce'},\n",
       " {'MAP@1': 0.03477402284752523,\n",
       "  'MAP@5': 0.06698163027041105,\n",
       "  'MAP@10': 0.07277966423896895,\n",
       "  'MIUF@1': 2.020424706174193,\n",
       "  'MIUF@5': 2.823113684972394,\n",
       "  'MIUF@10': 3.7625225192486647,\n",
       "  'Serendipity@1': 7.626538834693788e-06,\n",
       "  'Serendipity@5': 2.4311904032477694e-05,\n",
       "  'Serendipity@10': 3.577375322457377e-05,\n",
       "  'model': 'sasrec_next_action_gbce'},\n",
       " {'MAP@1': 0.03476408463323054,\n",
       "  'MAP@5': 0.06706105865754652,\n",
       "  'MAP@10': 0.07246719992626127,\n",
       "  'MIUF@1': 2.029366296672227,\n",
       "  'MIUF@5': 2.814538100018135,\n",
       "  'MIUF@10': 3.616757947914895,\n",
       "  'Serendipity@1': 8.864940805646943e-06,\n",
       "  'Serendipity@5': 2.7204123658154736e-05,\n",
       "  'Serendipity@10': 2.9214145318701434e-05,\n",
       "  'model': 'sasrec_next_action_softmax_casual'},\n",
       " {'MAP@1': 0.04001981445941568,\n",
       "  'MAP@5': 0.06678375449816408,\n",
       "  'MAP@10': 0.07485368631340723,\n",
       "  'MIUF@1': 4.2954472973798925,\n",
       "  'MIUF@5': 5.091198216891677,\n",
       "  'MIUF@10': 5.485630716990267,\n",
       "  'Serendipity@1': 0.0005120932240612023,\n",
       "  'Serendipity@5': 0.00044805495381661963,\n",
       "  'Serendipity@10': 0.000418576504004872,\n",
       "  'model': 'albert_softmax'},\n",
       " {'MAP@1': 0.038324434931823574,\n",
       "  'MAP@5': 0.06540184459496218,\n",
       "  'MAP@10': 0.07308646660517493,\n",
       "  'MIUF@1': 2.710455354861349,\n",
       "  'MIUF@5': 3.571952413079634,\n",
       "  'MIUF@10': 4.316234265249551,\n",
       "  'Serendipity@1': 5.971226878948834e-05,\n",
       "  'Serendipity@5': 9.578469543058569e-05,\n",
       "  'Serendipity@10': 0.0001223886704412776,\n",
       "  'model': 'albert_bce'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = albert_model_bce.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"albert_bce\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/amsemenov2/git/RecTools_origin/RecTools/examples/tutorials/../../rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.0 M \n",
      "---------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.884     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53105334b846441ebaa5d64b6296c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 26s, sys: 5.2 s, total: 5min 31s\n",
      "Wall time: 5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AlBERT4RecModel at 0x7fc7205dad90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "albert_model_gbce.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 10s, sys: 10min 27s, total: 12min 38s\n",
      "Wall time: 22.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'MAP@1': 0.0347556169412836,\n",
       "  'MAP@5': 0.0670784020904029,\n",
       "  'MAP@10': 0.07305790205735346,\n",
       "  'MIUF@1': 2.026684699022298,\n",
       "  'MIUF@5': 2.809957769256009,\n",
       "  'MIUF@10': 3.7957813994754876,\n",
       "  'Serendipity@1': 8.831319032679852e-06,\n",
       "  'Serendipity@5': 2.9488162768383078e-05,\n",
       "  'Serendipity@10': 4.3597166218751995e-05,\n",
       "  'model': 'sasrec_next_action_softmax'},\n",
       " {'MAP@1': 0.030642412703662827,\n",
       "  'MAP@5': 0.06354385798198152,\n",
       "  'MAP@10': 0.06939375803777235,\n",
       "  'MIUF@1': 2.1760997525872234,\n",
       "  'MIUF@5': 2.8881699041484103,\n",
       "  'MIUF@10': 3.6832272807506397,\n",
       "  'Serendipity@1': 1.0021156220458501e-05,\n",
       "  'Serendipity@5': 2.479979331575515e-05,\n",
       "  'Serendipity@10': 3.303582017928996e-05,\n",
       "  'model': 'sasrec_next_action_bce'},\n",
       " {'MAP@1': 0.03477402284752523,\n",
       "  'MAP@5': 0.06698163027041105,\n",
       "  'MAP@10': 0.07277966423896895,\n",
       "  'MIUF@1': 2.020424706174193,\n",
       "  'MIUF@5': 2.823113684972394,\n",
       "  'MIUF@10': 3.7625225192486647,\n",
       "  'Serendipity@1': 7.626538834693788e-06,\n",
       "  'Serendipity@5': 2.4311904032477694e-05,\n",
       "  'Serendipity@10': 3.577375322457377e-05,\n",
       "  'model': 'sasrec_next_action_gbce'},\n",
       " {'MAP@1': 0.03476408463323054,\n",
       "  'MAP@5': 0.06706105865754652,\n",
       "  'MAP@10': 0.07246719992626127,\n",
       "  'MIUF@1': 2.029366296672227,\n",
       "  'MIUF@5': 2.814538100018135,\n",
       "  'MIUF@10': 3.616757947914895,\n",
       "  'Serendipity@1': 8.864940805646943e-06,\n",
       "  'Serendipity@5': 2.7204123658154736e-05,\n",
       "  'Serendipity@10': 2.9214145318701434e-05,\n",
       "  'model': 'sasrec_next_action_softmax_casual'},\n",
       " {'MAP@1': 0.04001981445941568,\n",
       "  'MAP@5': 0.06678375449816408,\n",
       "  'MAP@10': 0.07485368631340723,\n",
       "  'MIUF@1': 4.2954472973798925,\n",
       "  'MIUF@5': 5.091198216891677,\n",
       "  'MIUF@10': 5.485630716990267,\n",
       "  'Serendipity@1': 0.0005120932240612023,\n",
       "  'Serendipity@5': 0.00044805495381661963,\n",
       "  'Serendipity@10': 0.000418576504004872,\n",
       "  'model': 'albert_softmax'},\n",
       " {'MAP@1': 0.038324434931823574,\n",
       "  'MAP@5': 0.06540184459496218,\n",
       "  'MAP@10': 0.07308646660517493,\n",
       "  'MIUF@1': 2.710455354861349,\n",
       "  'MIUF@5': 3.571952413079634,\n",
       "  'MIUF@10': 4.316234265249551,\n",
       "  'Serendipity@1': 5.971226878948834e-05,\n",
       "  'Serendipity@5': 9.578469543058569e-05,\n",
       "  'Serendipity@10': 0.0001223886704412776,\n",
       "  'model': 'albert_bce'},\n",
       " {'MAP@1': 0.010698347668639174,\n",
       "  'MAP@5': 0.016776874597345438,\n",
       "  'MAP@10': 0.01951166997542527,\n",
       "  'MIUF@1': 4.308965718550152,\n",
       "  'MIUF@5': 7.650483048772014,\n",
       "  'MIUF@10': 7.218644739164207,\n",
       "  'Serendipity@1': 2.776411296568374e-05,\n",
       "  'Serendipity@5': 5.1112939990281435e-05,\n",
       "  'Serendipity@10': 4.4417163903892505e-05,\n",
       "  'model': 'albert_gbce'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recos = albert_model_gbce.recommend(\n",
    "    users=test_users_sasrec, \n",
    "    dataset=dataset_no_features,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets=\"warn\"\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos[[\"user_id\", \"item_id\", \"rank\"]], test, train, catalog)\n",
    "metric_values[\"model\"] = \"albert_gbce\"\n",
    "features_results.append(metric_values)\n",
    "features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>loss_type</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>loss_type</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>loss_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.219992</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0</td>\n",
       "      <td>0.882489</td>\n",
       "      <td>bce</td>\n",
       "      <td>0</td>\n",
       "      <td>8.398360</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.369999</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724247</td>\n",
       "      <td>bce</td>\n",
       "      <td>1</td>\n",
       "      <td>8.379260</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17.939146</td>\n",
       "      <td>softmax</td>\n",
       "      <td>2</td>\n",
       "      <td>0.696630</td>\n",
       "      <td>bce</td>\n",
       "      <td>2</td>\n",
       "      <td>8.383474</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17.678568</td>\n",
       "      <td>softmax</td>\n",
       "      <td>3</td>\n",
       "      <td>0.677156</td>\n",
       "      <td>bce</td>\n",
       "      <td>3</td>\n",
       "      <td>8.377734</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17.439960</td>\n",
       "      <td>softmax</td>\n",
       "      <td>4</td>\n",
       "      <td>0.664593</td>\n",
       "      <td>bce</td>\n",
       "      <td>4</td>\n",
       "      <td>8.377232</td>\n",
       "      <td>gbce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss loss_type  epoch  train_loss loss_type  epoch  \\\n",
       "0      0   19.219992   softmax      0    0.882489       bce      0   \n",
       "1      1   18.369999   softmax      1    0.724247       bce      1   \n",
       "2      2   17.939146   softmax      2    0.696630       bce      2   \n",
       "3      3   17.678568   softmax      3    0.677156       bce      3   \n",
       "4      4   17.439960   softmax      4    0.664593       bce      4   \n",
       "\n",
       "   train_loss loss_type  \n",
       "0    8.398360      gbce  \n",
       "1    8.379260      gbce  \n",
       "2    8.383474      gbce  \n",
       "3    8.377734      gbce  \n",
       "4    8.377232      gbce  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_loss_df, _ = get_log_values(albert_model, is_val=False)\n",
    "softmax_loss_df[\"loss_type\"] = \"softmax\"\n",
    "bce_loss_df, _ = get_log_values(albert_model_bce, is_val=False)\n",
    "bce_loss_df[\"loss_type\"] = \"bce\"\n",
    "gbce_loss_df, _ = get_log_values(albert_model_gbce, is_val=False)\n",
    "gbce_loss_df[\"loss_type\"] = \"gbce\"\n",
    "pd.concat([softmax_loss_df, bce_loss_df, gbce_loss_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAP@1</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>MIUF@1</th>\n",
       "      <th>MIUF@5</th>\n",
       "      <th>MIUF@10</th>\n",
       "      <th>Serendipity@1</th>\n",
       "      <th>Serendipity@5</th>\n",
       "      <th>Serendipity@10</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034756</td>\n",
       "      <td>0.067078</td>\n",
       "      <td>0.073058</td>\n",
       "      <td>2.026685</td>\n",
       "      <td>2.809958</td>\n",
       "      <td>3.795781</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>sasrec_next_action_softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030642</td>\n",
       "      <td>0.063544</td>\n",
       "      <td>0.069394</td>\n",
       "      <td>2.176100</td>\n",
       "      <td>2.888170</td>\n",
       "      <td>3.683227</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>sasrec_next_action_bce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034774</td>\n",
       "      <td>0.066982</td>\n",
       "      <td>0.072780</td>\n",
       "      <td>2.020425</td>\n",
       "      <td>2.823114</td>\n",
       "      <td>3.762523</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>sasrec_next_action_gbce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034764</td>\n",
       "      <td>0.067061</td>\n",
       "      <td>0.072467</td>\n",
       "      <td>2.029366</td>\n",
       "      <td>2.814538</td>\n",
       "      <td>3.616758</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>sasrec_next_action_softmax_casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040020</td>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>4.295447</td>\n",
       "      <td>5.091198</td>\n",
       "      <td>5.485631</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>albert_softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.038324</td>\n",
       "      <td>0.065402</td>\n",
       "      <td>0.073086</td>\n",
       "      <td>2.710455</td>\n",
       "      <td>3.571952</td>\n",
       "      <td>4.316234</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>albert_bce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>4.308966</td>\n",
       "      <td>7.650483</td>\n",
       "      <td>7.218645</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>albert_gbce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MAP@1     MAP@5    MAP@10    MIUF@1    MIUF@5   MIUF@10  Serendipity@1  \\\n",
       "0  0.034756  0.067078  0.073058  2.026685  2.809958  3.795781       0.000009   \n",
       "1  0.030642  0.063544  0.069394  2.176100  2.888170  3.683227       0.000010   \n",
       "2  0.034774  0.066982  0.072780  2.020425  2.823114  3.762523       0.000008   \n",
       "3  0.034764  0.067061  0.072467  2.029366  2.814538  3.616758       0.000009   \n",
       "4  0.040020  0.066784  0.074854  4.295447  5.091198  5.485631       0.000512   \n",
       "5  0.038324  0.065402  0.073086  2.710455  3.571952  4.316234       0.000060   \n",
       "6  0.010698  0.016777  0.019512  4.308966  7.650483  7.218645       0.000028   \n",
       "\n",
       "   Serendipity@5  Serendipity@10                              model  \n",
       "0       0.000029        0.000044         sasrec_next_action_softmax  \n",
       "1       0.000025        0.000033             sasrec_next_action_bce  \n",
       "2       0.000024        0.000036            sasrec_next_action_gbce  \n",
       "3       0.000027        0.000029  sasrec_next_action_softmax_casual  \n",
       "4       0.000448        0.000419                     albert_softmax  \n",
       "5       0.000096        0.000122                         albert_bce  \n",
       "6       0.000051        0.000044                        albert_gbce  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics = pd.DataFrame(features_results)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `n_blocks * n_inner_groups`,     `n_hidden_groups * n_inner_groups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_blocks = 11\n",
    "# n_hidden_groups = 2\n",
    "# n_inner_groups = 3\n",
    "# nb nhg nig\n",
    "# 0   0   0\n",
    "#         1\n",
    "#         2\n",
    "# 1   0   0\n",
    "#         1\n",
    "#         2\n",
    "# 2   0   0\n",
    "#         1\n",
    "#         2\n",
    "# 3   0   0\n",
    "#         1\n",
    "#         2\n",
    "# 4   0   0\n",
    "#         1\n",
    "#         2\n",
    "# 5   0   3\n",
    "#         4\n",
    "#         5\n",
    "# 6   1   3\n",
    "#         4\n",
    "#         5\n",
    "# 7   1   3\n",
    "#         4\n",
    "#         5\n",
    "# 8   1   3\n",
    "#         4\n",
    "#         5\n",
    "# 9   1   3\n",
    "#         4\n",
    "#         5\n",
    "# 10  1   3\n",
    "#         4\n",
    "#         5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
