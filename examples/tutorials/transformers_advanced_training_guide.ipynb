{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Models Advanced Training Guide\n",
    "This guide is showing advanced features of RecTools transformer models training.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* Prepare data\n",
    "* Advanced training guide\n",
    "    * Validation fold\n",
    "    * Validation loss\n",
    "    * Callback for Early Stopping\n",
    "    * Callbacks for Checkpoints (+ loading checkpoints)\n",
    "    * Callbacks for RecSys metrics (+ checkpoints on RecSys metrics)\n",
    "* Advanced training full example\n",
    "    * Running full training with all of the described validation features on Kion dataset\n",
    "* More RecTools features for transformers\n",
    "    * Saving and loading models\n",
    "    * Configs for transformer models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import typing as tp\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "from rectools import Columns, ExternalIds\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.metrics import NDCG, Recall, Serendipity, calc_metrics\n",
    "from rectools.models import BERT4RecModel, SASRecModel, load_model\n",
    "from rectools.models.nn.item_net import IdEmbeddingsItemNet\n",
    "from rectools.models.nn.transformer_base import TransformerModelBase\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data_en.zip\n",
      "  inflating: data_en/items_en.csv    \n",
      "  inflating: __MACOSX/data_en/._items_en.csv  \n",
      "  inflating: data_en/interactions.csv  \n",
      "  inflating: __MACOSX/data_en/._interactions.csv  \n",
      "  inflating: data_en/users_en.csv    \n",
      "  inflating: __MACOSX/data_en/._users_en.csv  \n",
      "CPU times: user 107 ms, sys: 42.1 ms, total: 149 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wget -q https://github.com/irsafilo/KION_DATASET/raw/f69775be31fa5779907cf0a92ddedb70037fb5ae/data_en.zip -O data_en.zip\n",
    "!unzip -o data_en.zip\n",
    "!rm data_en.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476251, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>watched_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176549</td>\n",
       "      <td>9506</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>4250</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>699317</td>\n",
       "      <td>1659</td>\n",
       "      <td>2021-05-29</td>\n",
       "      <td>8317</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id   datetime  total_dur  watched_pct\n",
       "0   176549     9506 2021-05-11       4250         72.0\n",
       "1   699317     1659 2021-05-29       8317        100.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download dataset\n",
    "DATA_PATH = Path(\"./data_en\")\n",
    "items = pd.read_csv(DATA_PATH / 'items_en.csv', index_col=0)\n",
    "interactions = (\n",
    "    pd.read_csv(DATA_PATH / 'interactions.csv', parse_dates=[\"last_watch_dt\"])\n",
    "    .rename(columns={\"last_watch_dt\": Columns.Datetime})\n",
    ")\n",
    "\n",
    "print(interactions.shape)\n",
    "interactions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962179, 15706)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions[Columns.User].nunique(), interactions[Columns.Item].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476251, 4)\n"
     ]
    }
   ],
   "source": [
    "# Process interactions\n",
    "interactions[Columns.Weight] = np.where(interactions['watched_pct'] > 10, 3, 1)\n",
    "raw_interactions = interactions[[\"user_id\", \"item_id\", \"datetime\", \"weight\"]]\n",
    "print(raw_interactions.shape)\n",
    "raw_interactions.head(2)\n",
    "\n",
    "dataset = Dataset.construct(raw_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE=60\n",
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation fold\n",
    "\n",
    "Models do not create validation fold during `fit` by default. However, there is a simple way to force it.\n",
    "\n",
    "Let's assume that we want to use Leave-One-Out validation for specific set of users. To apply it we need to implement `get_val_mask_func` with required logic and pass it to model during initialization. \n",
    "\n",
    "This function should receive interactions with standard RecTools columns and return a binary mask which identifies interactions that should not be used during model training. But instrad should be used for validation loss calculation. They will also be available for Lightning Callbacks to allow RecSys metrics computations.\n",
    "\n",
    "*Please make sure you do not use `partial` while doing this. Partial functions cannot be by serialized using RecTools.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement `get_val_mask_func`\n",
    "\n",
    "N_VAL_USERS = 2048\n",
    "unique_users = raw_interactions[Columns.User].unique()\n",
    "VAL_USERS = unique_users[: N_VAL_USERS]\n",
    "\n",
    "def leave_one_out_mask_for_users(interactions: pd.DataFrame, val_users: ExternalIds) -> np.ndarray:\n",
    "    rank = (\n",
    "        interactions\n",
    "        .sort_values(Columns.Datetime, ascending=False, kind=\"stable\")\n",
    "        .groupby(Columns.User, sort=False)\n",
    "        .cumcount()\n",
    "    )\n",
    "    val_mask = (\n",
    "        (interactions[Columns.User].isin(val_users))\n",
    "        & (rank == 0)\n",
    "    )\n",
    "    return val_mask.values\n",
    "\n",
    "# We do not use `partial` for correct serialization of the model\n",
    "def get_val_mask_func(interactions: pd.DataFrame):\n",
    "    return leave_one_out_mask_for_users(interactions, val_users = VAL_USERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = SASRecModel(\n",
    "    n_factors=64,\n",
    "    n_blocks=2,\n",
    "    n_heads=2,\n",
    "    dropout_rate=0.2,\n",
    "    train_min_user_interactions=5,\n",
    "    session_max_len=50,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet,),\n",
    "    get_val_mask_func=get_val_mask_func,  # pass our custom `get_val_mask_func`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation loss\n",
    "\n",
    "Let's check how the validation loss is being logged.\n",
    "We just want to quickly check functionality for now so let's create a custom Lightning trainer for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                           | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | torch_model | TransformerBasedSessionEncoder | 987 K  | train\n",
      "-----------------------------------------------------------------------\n",
      "987 K     Trainable params\n",
      "0         Non-trainable params\n",
      "987 K     Total params\n",
      "3.951     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ca327bf3794f16b51a88b77725f19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f506ed54be486bbd3b71463edefb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aabdc8fbf274d27987b4ab665549e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297f7dd5d03b4b6a91ee8a03dacdedb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x296553d60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='cpu',  # TODO: change\n",
    "    devices=1,\n",
    "    min_epochs=2,\n",
    "    max_epochs=2, \n",
    "    deterministic=True,\n",
    "    limit_train_batches=2,  # use only 2 batches for each epoch for a test run\n",
    "    enable_checkpointing=False,\n",
    "    logger = CSVLogger(\"test_logs\")\n",
    ")\n",
    "\n",
    "# Replace default trainer with our custom one\n",
    "model._trainer = trainer\n",
    "\n",
    "# Fit model. Validation fold and validation loss computation will be done under the hood.\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at model logs. We can access logs directory with `model.fit_trainer.log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hparams.yaml metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# What's inside the logs directory?\n",
    "!ls $model.fit_trainer.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,step,train_loss,val_loss\n",
      "0,1,,22.41293716430664\n",
      "0,1,22.974777221679688,\n",
      "1,3,,22.27031898498535\n",
      "1,3,22.650423049926758,\n"
     ]
    }
   ],
   "source": [
    "# Losses and metrics are in the `metrics.csv`\n",
    "# Let's look at logs\n",
    "\n",
    "!tail $model.fit_trainer.log_dir/metrics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback for Early Stopping\n",
    "\n",
    "Now that we have validation loss logged, let's use ot for model Early Stopping. It will ensure that model will not resume training if validation loss (or any other custom metric) doesn't impove. We have Lightning Callbacks for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=SASRecModel.val_loss_name,   # or just pass \"val_loss\" here\n",
    "    mode=\"min\",\n",
    "    min_delta=1.  # just for a quick test of functionality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                           | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | torch_model | TransformerBasedSessionEncoder | 987 K  | train\n",
      "-----------------------------------------------------------------------\n",
      "987 K     Trainable params\n",
      "0         Non-trainable params\n",
      "987 K     Total params\n",
      "3.951     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e615856eea8497bb901ffe3954ed8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d67a6837f40450f97c1026a7deaf607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb6fe0cdabd4f11921fe1b7b05a3178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486a6b64bbee426bbd9c567027e6b551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6112952e6f4b618377856b93c7912a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89452783c7b04337b831fe076371f474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x296553d60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='cpu',  # TODO: change\n",
    "    devices=1,\n",
    "    min_epochs=1,  # minimum number of epochs to train before early stopping\n",
    "    max_epochs=20,  # maximum number of epochs to train\n",
    "    deterministic=True,\n",
    "    limit_train_batches=2,  # use only 2 batches for each epoch for a test run\n",
    "    enable_checkpointing=False,\n",
    "    logger = CSVLogger(\"test_logs\"),\n",
    "    callbacks=early_stopping_callback,  # pass our callback\n",
    ")\n",
    "\n",
    "# Replace default trainer with our custom one\n",
    "model._trainer = trainer\n",
    "\n",
    "# Fit model. Everything will happen under the hood\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here model stopped training after 4 epochs because validation loss wasn't improving by our specified `min_delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,step,train_loss,val_loss\n",
      "0,1,,22.35995864868164\n",
      "0,1,22.873361587524414,\n",
      "1,3,,22.200777053833008\n",
      "1,3,22.538841247558594,\n",
      "2,5,,21.98937225341797\n",
      "2,5,22.36414909362793,\n",
      "3,7,,21.726999282836914\n",
      "3,7,22.734487533569336,\n"
     ]
    }
   ],
   "source": [
    "# Let's check out logs\n",
    "!tail $model.fit_trainer.log_dir/metrics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback for Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint after last epoch\n",
    "last_epoch_ckpt = ModelCheckpoint(filename=\"last_epoch\")\n",
    "\n",
    "# Checkpoints based on validation loss\n",
    "least_val_loss_ckpt = ModelCheckpoint(\n",
    "    monitor=SASRecModel.val_loss_name,   # or just pass \"val_loss\" here,\n",
    "    mode=\"min\",\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",\n",
    "    save_top_k=2,  # Let's save top 2 checkpoints for validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                           | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | torch_model | TransformerBasedSessionEncoder | 987 K  | train\n",
      "-----------------------------------------------------------------------\n",
      "987 K     Trainable params\n",
      "0         Non-trainable params\n",
      "987 K     Total params\n",
      "3.951     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344a3cc3f3bb4a1c8614b33ed46e315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c4c2382ad444db9ae44fef8e66e28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd606418056473a9ef3b52c4b71dd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a477685b5aa644909e2c42c20b448167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8f39ee47eb411d8829f3368889f317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6a124643ef4436a5cfbe74b642478f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dcc3ac816d4ee79bd6cb8c8572efe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe6967a86fa4476a0653fa7f78f1c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x296553d60>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='cpu',  # TODO: change\n",
    "    devices=1,\n",
    "    min_epochs=1,\n",
    "    max_epochs=6,\n",
    "    deterministic=True,\n",
    "    limit_train_batches=2,  # use only 2 batches for each epoch for a test run\n",
    "    logger = CSVLogger(\"test_logs\"),\n",
    "    callbacks=[last_epoch_ckpt, least_val_loss_ckpt],  # pass our callbacks for checkpoints\n",
    ")\n",
    "\n",
    "# Replace default trainer with our custom one\n",
    "model._trainer = trainer\n",
    "\n",
    "# Fit model. Everything will happen under the hood\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at model checkpoints that were saved. By default they are neing saved to `checkpoints` directory in  `model.fit_trainer.log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4-val_loss=21.46.ckpt last_epoch.ckpt\n",
      "epoch=5-val_loss=21.16.ckpt\n"
     ]
    }
   ],
   "source": [
    "# We have 2 checkpoints for 2 best validation loss values and one for last epoch\n",
    "!ls $model.fit_trainer.log_dir/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading checkpoints is very simple with `load_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11938674"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"temp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "loaded = SASRecModel.load(\"temp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[43mSASRecModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/checkpoints/last_epoch.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_project/metrics/RecTools/rectools/models/base.py:266\u001b[0m, in \u001b[0;36mModelBase.load\u001b[0;34m(cls, f)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mLoad model from file.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    Model instance.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m data \u001b[38;5;241m=\u001b[39m read_bytes(f)\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_project/metrics/RecTools/rectools/models/base.py:288\u001b[0m, in \u001b[0;36mModelBase.loads\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloads\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data: \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tpe\u001b[38;5;241m.\u001b[39mSelf:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Load model from bytes.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m        If loaded object is not a direct instance of model class.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loaded\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded object is not a direct instance of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "loaded = SASRecModel.load(model.fit_trainer.log_dir + \"/checkpoints/last_epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks for RecSys metrics during training\n",
    "\n",
    "Monitoring RecSys metrics (or any other custom things) on validation fold is not available out of the box, but we can create a custom Lightning Callback for that.\n",
    "\n",
    "Below is an example of calculating standard RecTools metrics on validation fold during training. We use it as an explicit example that any customization is possible. But it is recommend to implement metrics calculation using `torch` for faster computations.\n",
    "\n",
    "Please look at PyTorch Lightning documentation for more details on custom callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom Callback for RecTools metrics computation within validation epochs during training.\n",
    "\n",
    "class ValidationMetrics(Callback):\n",
    "    \n",
    "    def __init__(self, top_k: int, val_metrics: tp.Dict, verbose: int = 0) -> None:\n",
    "        self.top_k = top_k\n",
    "        self.val_metrics = val_metrics\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.epoch_n_users: int = 0\n",
    "        self.batch_metrics: tp.List[tp.Dict[str, float]] = []\n",
    "\n",
    "    def on_validation_batch_end(\n",
    "        self, \n",
    "        trainer: Trainer, \n",
    "        pl_module: LightningModule, \n",
    "        outputs: tp.Dict[str, torch.Tensor], \n",
    "        batch: tp.Dict[str, torch.Tensor], \n",
    "        batch_idx: int, \n",
    "        dataloader_idx: int = 0\n",
    "    ) -> None:\n",
    "        logits = outputs[\"logits\"]\n",
    "        if logits is None:\n",
    "            logits = pl_module.torch_model.encode_sessions(batch[\"x\"], pl_module.item_embs)[:, -1, :]\n",
    "        _, sorted_batch_recos = logits.topk(k=self.top_k)\n",
    "\n",
    "        batch_recos = sorted_batch_recos.tolist()\n",
    "        targets = batch[\"y\"].tolist()\n",
    "\n",
    "        batch_val_users = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                itertools.repeat(idx, len(recos)) for idx, recos in enumerate(batch_recos)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        batch_target_users = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                itertools.repeat(idx, len(targets)) for idx, targets in enumerate(targets)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        batch_recos_df = pd.DataFrame(\n",
    "            {\n",
    "                Columns.User: batch_val_users,\n",
    "                Columns.Item: list(itertools.chain.from_iterable(batch_recos)),\n",
    "            }\n",
    "        )\n",
    "        batch_recos_df[Columns.Rank] = batch_recos_df.groupby(Columns.User, sort=False).cumcount() + 1\n",
    "\n",
    "        interactions = pd.DataFrame(\n",
    "            {\n",
    "                Columns.User: batch_target_users,\n",
    "                Columns.Item: list(itertools.chain.from_iterable(targets)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        prev_interactions = pl_module.data_preparator.train_dataset.interactions.df\n",
    "        catalog = prev_interactions[Columns.Item].unique()\n",
    "\n",
    "        batch_metrics = calc_metrics(\n",
    "            self.val_metrics, \n",
    "            batch_recos_df,\n",
    "            interactions, \n",
    "            prev_interactions,\n",
    "            catalog\n",
    "        )\n",
    "\n",
    "        batch_n_users = batch[\"x\"].shape[0]\n",
    "        self.batch_metrics.append({metric: value * batch_n_users for metric, value in batch_metrics.items()})\n",
    "        self.epoch_n_users += batch_n_users\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n",
    "        epoch_metrics = dict(sum(map(Counter, self.batch_metrics), Counter()))\n",
    "        epoch_metrics = {metric: value / self.epoch_n_users for metric, value in epoch_metrics.items()}\n",
    "\n",
    "        self.log_dict(epoch_metrics, on_step=False, on_epoch=True, prog_bar=self.verbose > 0)\n",
    "\n",
    "        self.batch_metrics.clear()\n",
    "        self.epoch_n_users = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When custom metrics callback is implemented, we can use the values of these metrics for both Early Stopping and Checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize callbacks for metrics calculation and checkpoint based on NDCG value\n",
    "\n",
    "metrics = {\n",
    "    \"NDCG@10\": NDCG(k=10),\n",
    "    \"Recall@10\": Recall(k=10),\n",
    "    \"Serendipity@10\": Serendipity(k=10),\n",
    "}\n",
    "top_k = max([metric.k for metric in metrics.values()])\n",
    "\n",
    "# Callback for calculating RecSys metrics\n",
    "val_metrics_callback = ValidationMetrics(top_k=top_k, val_metrics=metrics, verbose=1)\n",
    "\n",
    "# Callback for checkpoint based on maximization of NDCG@10\n",
    "best_ndcg_ckpt = ModelCheckpoint(\n",
    "    monitor=\"NDCG@10\",\n",
    "    mode=\"max\",\n",
    "    filename=\"{epoch}-{NDCG@10:.2f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                           | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | torch_model | TransformerBasedSessionEncoder | 987 K  | train\n",
      "-----------------------------------------------------------------------\n",
      "987 K     Trainable params\n",
      "0         Non-trainable params\n",
      "987 K     Total params\n",
      "3.951     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49881906f144cb9a5f3ec9f1a36c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd88091fca404c36b2fad769e7257688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a48a9f03794ab9a4e55abc65b85c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877ea612efa94b0483e72f0371684cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bdd58938ee438486fa6bf3453f2e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f184f15d2c48638b90a8c7afb33ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a364a7b6fa084dfeb75e88d7d1838d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7575b91420a844a883471c041f3027f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x296553d60>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='cpu',  # TODO: change\n",
    "    devices=1,\n",
    "    min_epochs=1,\n",
    "    max_epochs=6,\n",
    "    deterministic=True,\n",
    "    limit_train_batches=2,  # use only 2 batches for each epoch for a test run\n",
    "    logger = CSVLogger(\"test_logs\"),\n",
    "    callbacks=[val_metrics_callback, best_ndcg_ckpt],  # pass our callbacks\n",
    ")\n",
    "\n",
    "# Replace default trainer with our custom one\n",
    "model._trainer = trainer\n",
    "\n",
    "# Fit model. Everything will happen under the hood\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checkpoint for best NDCG@10 model in the usual directory for checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5-NDCG@10=0.01.ckpt\n"
     ]
    }
   ],
   "source": [
    "!ls $model.fit_trainer.log_dir/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also now have metrics in our logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10,Recall@10,Serendipity@10,epoch,step,train_loss,val_loss\n",
      "0.0006136037991382182,0.005259697791188955,4.036495283799013e-06,0,1,,22.36232566833496\n",
      ",,,0,1,22.85256004333496,\n",
      "0.00378932966850698,0.04470742866396904,5.4826059567858465e-06,1,3,,22.194259643554688\n",
      ",,,1,3,22.471229553222656,\n",
      "0.004971458576619625,0.048652201890945435,5.865532330062706e-06,2,5,,21.967544555664062\n",
      ",,,2,5,22.728843688964844,\n",
      "0.008074083365499973,0.04996712505817413,5.288889951771125e-06,3,7,,21.701507568359375\n",
      ",,,3,7,22.52100372314453,\n",
      "0.010768753476440907,0.0788954645395279,3.748174322026898e-06,4,9,,21.411954879760742\n"
     ]
    }
   ],
   "source": [
    "!head $model.fit_trainer.log_dir/metrics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load them to read more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.852560</td>\n",
       "      <td>22.362326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22.471230</td>\n",
       "      <td>22.194260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22.728844</td>\n",
       "      <td>21.967545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>22.521004</td>\n",
       "      <td>21.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>22.202381</td>\n",
       "      <td>21.411955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss   val_loss\n",
       "0      0   22.852560  22.362326\n",
       "1      1   22.471230  22.194260\n",
       "2      2   22.728844  21.967545\n",
       "3      3   22.521004  21.701508\n",
       "4      4   22.202381  21.411955"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_logs(model: TransformerModelBase) -> tp.Tuple[pd.DataFrame, ...]:\n",
    "    log_path = Path(model.fit_trainer.log_dir) / \"metrics.csv\"\n",
    "    epoch_metrics_df = pd.read_csv(log_path)\n",
    "    \n",
    "    loss_df = epoch_metrics_df[[\"epoch\", \"train_loss\"]].dropna()\n",
    "    val_loss_df = epoch_metrics_df[[\"epoch\", \"val_loss\"]].dropna()\n",
    "    loss_df = pd.merge(loss_df, val_loss_df, how=\"inner\", on=\"epoch\")\n",
    "    loss_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    metrics_df = epoch_metrics_df.drop(columns=[\"train_loss\", \"val_loss\"]).dropna()\n",
    "    metrics_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return loss_df, metrics_df\n",
    "\n",
    "loss_df, metrics_df = get_logs(model)\n",
    "\n",
    "loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Serendipity@10</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.044707</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.048652</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.078895</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NDCG@10  Recall@10  Serendipity@10  epoch  step\n",
       "0  0.000614   0.005260        0.000004      0     1\n",
       "1  0.003789   0.044707        0.000005      1     3\n",
       "2  0.004971   0.048652        0.000006      2     5\n",
       "3  0.008074   0.049967        0.000005      3     7\n",
       "4  0.010769   0.078895        0.000004      4     9"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced training full example\n",
    "Running full training with all of the described validation features on Kion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASRecModel(\n",
    "    n_factors=64,\n",
    "    n_blocks=2,\n",
    "    n_heads=2,\n",
    "    dropout_rate=0.2,\n",
    "    train_min_user_interactions=5,\n",
    "    session_max_len=50,\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet,),\n",
    "    get_val_mask_func=get_val_mask_func,  # pass our custom `get_val_mask_func`\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=SASRecModel.val_loss_name,   # or just pass \"val_loss\" here\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cpu',  # TODO: change\n",
    "    devices=1,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    deterministic=True,\n",
    "    logger = CSVLogger(\"sasrec_logs\"),\n",
    "    callbacks=[\n",
    "        val_metrics_callback,  # calculate RecSys metrics\n",
    "        best_ndcg_ckpt,  # save best NDCG model checkpoint\n",
    "        last_epoch_ckpt,  # save model checkpoint after the last train epoch\n",
    "        early_stopping_callback,  # early stopping on validation loss\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Replace default trainer with our custom one\n",
    "model._trainer = trainer\n",
    "\n",
    "# Fit model. Everything will happen under the hood\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df, metrics_df = get_logs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $model.fit_trainer.log_dir/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
