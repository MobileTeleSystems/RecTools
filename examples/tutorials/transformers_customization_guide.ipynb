{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import typing as tp\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "import typing_extensions as tpe\n",
    "\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.models import BERT4RecModel, SASRecModel\n",
    "\n",
    "\n",
    "from rectools.dataset.dataset import Dataset, DatasetSchema\n",
    "from rectools.models.nn.item_net import (\n",
    "    ItemNetBase,\n",
    "    SumOfEmbeddingsConstructor,\n",
    ")\n",
    "from rectools.models.nn.transformer_net_blocks import (\n",
    "    PreLNTransformerLayer,\n",
    "    TransformerLayersBase,\n",
    ")\n",
    "from rectools.models.nn.constants import InitKwargs\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !wget -q https://github.com/irsafilo/KION_DATASET/raw/f69775be31fa5779907cf0a92ddedb70037fb5ae/data_original.zip -O data_original.zip\n",
    "# !unzip -o data_original.zip\n",
    "# !rm data_original.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data_original\")\n",
    "\n",
    "interactions = (\n",
    "    pd.read_csv(DATA_PATH / 'interactions.csv', parse_dates=[\"last_watch_dt\"])\n",
    "    .rename(columns={\"last_watch_dt\": \"datetime\"})\n",
    ")\n",
    "interactions[Columns.Weight] = np.where(interactions['watched_pct'] > 10, 3, 1)\n",
    "dataset_no_features = Dataset.construct(\n",
    "    interactions_df=interactions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE=60\n",
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get custom trainer for quick debugging\n",
    "def get_debug_trainer() -> Trainer:\n",
    "    return Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[1],\n",
    "        min_epochs=1,\n",
    "        max_epochs=1,\n",
    "        deterministic=True,\n",
    "        limit_train_batches=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2205.04507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from rectools.models.nn.bert4rec import BERT4RecDataPreparator\n",
    "from rectools.models.nn.constants import MASKING_VALUE\n",
    "from rectools.models.nn.transformer_lightning import TransformerLightningModule\n",
    "\n",
    "\n",
    "# \"MASK\" token is used for predicting one next token.\n",
    "class NextItemDataPreparator(BERT4RecDataPreparator):\n",
    "    \n",
    "    def _collate_fn_train(\n",
    "        self,\n",
    "        batch: List[Tuple[List[int], List[float]]],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Truncate each session from right to keep `session_max_len` items.\n",
    "        Do left padding until `session_max_len` is reached.\n",
    "        Split to `x`, `y`, and `yw`.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        x = np.zeros((batch_size, self.session_max_len))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        yw = np.zeros((batch_size, 1))\n",
    "        for i, (ses, ses_weights) in enumerate(batch):\n",
    "            session = ses.copy()\n",
    "            session[-1] = self.extra_token_ids[MASKING_VALUE]\n",
    "            x[i, -len(ses) :] = session  # ses: [session_len] -> x[i]: [session_max_len]\n",
    "            y[i] = ses[-1]  # ses: [session_len] -> y[i]: [1]\n",
    "            yw[i] = ses_weights[-1]  # ses_weights: [session_len] -> yw[i]: [1]\n",
    "\n",
    "        batch_dict = {\"x\": torch.LongTensor(x), \"y\": torch.LongTensor(y), \"yw\": torch.FloatTensor(yw)}\n",
    "        if self.n_negatives is not None:\n",
    "            negatives = torch.randint(\n",
    "                low=self.n_item_extra_tokens,\n",
    "                high=self.item_id_map.size,\n",
    "                size=(batch_size, 1, self.n_negatives),\n",
    "            )  # [batch_size, 1, n_negatives]\n",
    "            batch_dict[\"negatives\"] = negatives\n",
    "        return batch_dict\n",
    "\n",
    "\n",
    "# Last logits are used for reducing the number of calculations on training step.\n",
    "# You could also fill the y with zeros except for the last item in `_collate_fn_train`` and not change the training step \n",
    "class NextItemLightningModule(TransformerLightningModule):\n",
    "\n",
    "    def training_step(self, batch: tp.Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        x, y, w = batch[\"x\"], batch[\"y\"], batch[\"yw\"]\n",
    "        if self.loss == \"softmax\":\n",
    "            logits = self._get_full_catalog_logits(x)[:, -1: :]\n",
    "            loss = self._calc_softmax_loss(logits, y, w)\n",
    "        elif self.loss == \"BCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]\n",
    "            loss = self._calc_bce_loss(logits, y, w)\n",
    "        elif self.loss == \"gBCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]\n",
    "            loss = self._calc_gbce_loss(logits, y, w, negatives)\n",
    "        else:\n",
    "            loss = self._calc_custom_loss(batch, batch_idx)\n",
    "\n",
    "        self.log(self.train_loss_name, loss, on_step=False, on_epoch=True, prog_bar=self.verbose > 0)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "nextitem_transformer_bidirectional = BERT4RecModel(\n",
    "    data_preparator_type=NextItemDataPreparator,  # \"NextItem\" training objective data preparator\n",
    "    lightning_module_type=NextItemLightningModule,  # \"NextItem\" lightning module\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")\n",
    "\n",
    "nextitem_transformer_unidirectional = SASRecModel(\n",
    "    data_preparator_type=NextItemDataPreparator,   # \"NextItem\" training objective data preparator\n",
    "    lightning_module_type=NextItemLightningModule,  # \"NextItem\" lightning module\n",
    "    use_causal_attn=True,  # Apply causal attention mask\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dmtikhono1/RecTools/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 5.5 M  | train\n",
      "-----------------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "22.040    Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d0297542c7483dba27e03cd4ed2e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 s, sys: 3.3 s, total: 34.2 s\n",
      "Wall time: 27.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.bert4rec.BERT4RecModel at 0x7f3fd903bac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nextitem_transformer_bidirectional.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dmtikhono1/RecTools/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.7 M  | train\n",
      "-----------------------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.890    Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef281146a32431bb36965190283983d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 3.31 s, total: 32.2 s\n",
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x7f3fd903b550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nextitem_transformer_unidirectional.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT\n",
    "Albert has 2 main innovations which can be used together or separately:\n",
    "1. Learning embeddings of smaller size and then projecting them to the required size through a Liner projection\n",
    "2. Sharing weights between transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ---------- Special Albert logic for Embeddings ---------- ### #\n",
    "\n",
    "class AlBERT4RecSumOfEmbeddingsConstructor(SumOfEmbeddingsConstructor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_items: int,\n",
    "        n_factors: int,\n",
    "        item_net_blocks: tp.Sequence[ItemNetBase],\n",
    "        emb_factors: int = 16,  # accept new kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            n_items=n_items,\n",
    "            item_net_blocks=item_net_blocks,\n",
    "        )\n",
    "        self.item_emb_proj = nn.Linear(emb_factors, n_factors)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: Dataset,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "        emb_factors: int,  # accept new kwargs\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset.item_id_map.size\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset(dataset, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, n_factors, item_net_blocks, emb_factors)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_schema(\n",
    "        cls,\n",
    "        dataset_schema: DatasetSchema,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "        emb_factors: int,  # accept new kwargs\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset_schema.items.n_hot\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset_schema(dataset_schema, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, n_factors, item_net_blocks, emb_factors)\n",
    "\n",
    "    def forward(self, items: torch.Tensor) -> torch.Tensor:\n",
    "        item_embs = super().forward(items)\n",
    "        item_embs = self.item_emb_proj(item_embs)\n",
    "        return item_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ---------- Special Albert logic for Transfromer Layers ---------- ### #\n",
    "    \n",
    "class AlBERT4RecPreLNTransformerLayers(TransformerLayersBase):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks: int,\n",
    "        n_factors: int,\n",
    "        n_heads: int,\n",
    "        dropout_rate: float,\n",
    "        ff_factors_multiplier: int = 4,\n",
    "        n_hidden_groups: int=1,  # accept new kwarg\n",
    "        n_inner_groups: int=1,  # accept new kwarg\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        n_fitted_blocks = int(n_hidden_groups * n_inner_groups)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                PreLNTransformerLayer(\n",
    "                    # number of encoder layer (AlBERTLayers)\n",
    "                    # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L428\n",
    "                    n_factors,\n",
    "                    n_heads,\n",
    "                    dropout_rate,\n",
    "                    ff_factors_multiplier,\n",
    "                )\n",
    "                # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L469\n",
    "                for _ in range(n_fitted_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.n_layers_per_group = n_blocks / n_hidden_groups\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        seqs: torch.Tensor,\n",
    "        timeline_mask: torch.Tensor,\n",
    "        attn_mask: tp.Optional[torch.Tensor],\n",
    "        key_padding_mask: tp.Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        for block_idx in range(self.n_blocks):\n",
    "            group_idx = int(block_idx / self.n_layers_per_group)\n",
    "            for inner_layer_idx in range(self.n_inner_groups):\n",
    "                layer_idx = group_idx * self.n_inner_groups + inner_layer_idx\n",
    "                seqs = self.transformer_blocks[block_idx](seqs, attn_mask, key_padding_mask)\n",
    "        return seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALBERT_ITEM_NET_CONSTRUCTOR_KWARGS = {  # these arguments are obligatory for our custom classes\n",
    "    \"emb_factors\": 32,\n",
    "}\n",
    "\n",
    "ALBERT_TRANSFORMER_LAYERS_KWARGS = {  # these arguments are obligatory for our custom classes\n",
    "    \"n_hidden_groups\": 2,\n",
    "    \"n_inner_groups\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "albert_model = BERT4RecModel(\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,  # custom item net constructor\n",
    "    item_net_constructor_kwargs=ALBERT_ITEM_NET_CONSTRUCTOR_KWARGS,  # kwargs for custom constructor\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,  # custom transformer layers\n",
    "    transformer_layers_kwargs=ALBERT_TRANSFORMER_LAYERS_KWARGS, # kwargs for custom transformer layers\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dmtikhono1/RecTools/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.1 M  | train\n",
      "-----------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.407     Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebe81c85dbb482180d21be64d4d7411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.6 s, sys: 4.36 s, total: 36 s\n",
      "Wall time: 29.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.bert4rec.BERT4RecModel at 0x7f3cce308970>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "albert_model.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "alsasrec = SASRecModel(\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,  # custom item net constructor\n",
    "    item_net_constructor_kwargs=ALBERT_ITEM_NET_CONSTRUCTOR_KWARGS,  # kwargs for custom constructor\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,  # custom transformer layers\n",
    "    transformer_layers_kwargs=ALBERT_TRANSFORMER_LAYERS_KWARGS, # kwargs for custom transformer layers\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dmtikhono1/RecTools/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.1 M  | train\n",
      "-----------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.407     Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be15dcd434342a0b73efe8153e18774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57 s, sys: 5.67 s, total: 1min 2s\n",
      "Wall time: 39.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.sasrec.SASRecModel at 0x7f3d36ce42b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "alsasrec.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about NextTokenTransformer with Albert logic and causal attention?\n",
    "# Just because we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "next_action_albert_causal = BERT4RecModel(\n",
    "    item_net_constructor_type=AlBERT4RecSumOfEmbeddingsConstructor,  # custom item net constructor\n",
    "    item_net_constructor_kwargs=ALBERT_ITEM_NET_CONSTRUCTOR_KWARGS,  # kwargs for custom constructor\n",
    "    transformer_layers_type=AlBERT4RecPreLNTransformerLayers,  # custom transformer layers\n",
    "    transformer_layers_kwargs=ALBERT_TRANSFORMER_LAYERS_KWARGS, # kwargs for custom transformer layers\n",
    "    data_preparator_type=NextItemDataPreparator,  # custom data preparator\n",
    "    lightning_module_type=NextItemLightningModule,  # custom lightning module\n",
    "    use_causal_attn=True,  # Apply causal attention mask\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dmtikhono1/RecTools/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.1 M  | train\n",
      "-----------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.407     Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2068caad4b406d86f674af208d2a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 s, sys: 4.16 s, total: 51.6 s\n",
      "Wall time: 37.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.bert4rec.BERT4RecModel at 0x7f3c86767220>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next_action_albert_causal.fit(dataset_no_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about configs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = next_action_albert_causal.get_params(simple_types=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls': 'BERT4RecModel',\n",
       " 'verbose': 0,\n",
       " 'data_preparator_type': '__main__.NextItemDataPreparator',\n",
       " 'n_blocks': 2,\n",
       " 'n_heads': 4,\n",
       " 'n_factors': 256,\n",
       " 'use_pos_emb': True,\n",
       " 'use_causal_attn': True,\n",
       " 'use_key_padding_mask': True,\n",
       " 'dropout_rate': 0.2,\n",
       " 'session_max_len': 100,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'batch_size': 128,\n",
       " 'loss': 'softmax',\n",
       " 'n_negatives': 1,\n",
       " 'gbce_t': 0.2,\n",
       " 'lr': 0.001,\n",
       " 'epochs': 3,\n",
       " 'deterministic': False,\n",
       " 'recommend_batch_size': 256,\n",
       " 'recommend_device': None,\n",
       " 'recommend_n_threads': 0,\n",
       " 'recommend_use_torch_ranking': True,\n",
       " 'train_min_user_interactions': 2,\n",
       " 'item_net_block_types': ['rectools.models.nn.item_net.IdEmbeddingsItemNet',\n",
       "  'rectools.models.nn.item_net.CatFeaturesItemNet'],\n",
       " 'item_net_constructor_type': '__main__.AlBERT4RecSumOfEmbeddingsConstructor',\n",
       " 'pos_encoding_type': 'rectools.models.nn.transformer_net_blocks.LearnableInversePositionalEncoding',\n",
       " 'transformer_layers_type': '__main__.AlBERT4RecPreLNTransformerLayers',\n",
       " 'lightning_module_type': '__main__.NextItemLightningModule',\n",
       " 'get_val_mask_func': None,\n",
       " 'get_trainer_func': '__main__.get_debug_trainer',\n",
       " 'data_preparator_kwargs': None,\n",
       " 'transformer_layers_kwargs.n_hidden_groups': 2,\n",
       " 'transformer_layers_kwargs.n_inner_groups': 1,\n",
       " 'item_net_block_kwargs': None,\n",
       " 'item_net_constructor_kwargs.emb_factors': 32,\n",
       " 'pos_encoding_kwargs': None,\n",
       " 'lightning_module_kwargs': None,\n",
       " 'mask_prob': 0.15}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = BERT4RecModel.from_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dmtikhono1/RecTools/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 2.1 M  | train\n",
      "-----------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.407     Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5a9bb492a04e1fa58ea29faa47d69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.4 s, sys: 4.11 s, total: 40.5 s\n",
      "Wall time: 35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.bert4rec.BERT4RecModel at 0x7f3cfffecee0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(dataset_no_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rectools",
   "language": "python",
   "name": "rectools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
