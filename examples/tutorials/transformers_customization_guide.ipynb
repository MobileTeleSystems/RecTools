{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Models Customization Guide\n",
    "\n",
    "RecTools provides many options to change any part of the model with custom modules: from training objective to special transformer layers logic. Current guide provides just a few examples of the various customizations that can be done.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* Prepare data\n",
    "* \"Next Action\" training objective from Pinnerformer\n",
    "    - Custom data preparator and lightning module\n",
    "    - Create `NextActionTransformer`\n",
    "    - Enable unidirectional attention\n",
    "* ALBERT\n",
    "    - Custom transformer layers and item net constructor\n",
    "    - Pass ALBERT modules to `BERT4RecModel`\n",
    "    - Pass ALBERT modules to `SASRecModel`\n",
    "* How about `NextActionTransformer` with ALBERT modules and causal attention?\n",
    "    - Combining custom modules together\n",
    "* Cross-validation\n",
    "* Configs support for custom models\n",
    "* Full list of customization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as tp\n",
    "import typing_extensions as tpe\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.models import BERT4RecModel, SASRecModel, PopularModel\n",
    "from rectools.dataset.dataset import DatasetSchema\n",
    "from rectools.model_selection import TimeRangeSplitter, cross_validate\n",
    "from rectools.metrics import (\n",
    "    MAP,\n",
    "    CoveredUsers,\n",
    "    AvgRecPopularity,\n",
    "    Intersection,\n",
    "    HitRate,\n",
    "    Serendipity,\n",
    ")\n",
    "from rectools.models.nn.item_net import (\n",
    "    ItemNetBase,\n",
    "    SumOfEmbeddingsConstructor,\n",
    ")\n",
    "from rectools.models.nn.transformers.net_blocks import (\n",
    "    PreLNTransformerLayer,\n",
    "    TransformerLayersBase,\n",
    ")\n",
    "from rectools.models.nn.transformers.constants import MASKING_VALUE\n",
    "from rectools.models.nn.transformers.bert4rec import BERT4RecDataPreparator\n",
    "from rectools.models.nn.transformers.lightning import TransformerLightningModule\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "!wget -q https://github.com/irsafilo/KION_DATASET/raw/f69775be31fa5779907cf0a92ddedb70037fb5ae/data_original.zip -O data_original.zip\n",
    "!unzip -o data_original.zip\n",
    "!rm data_original.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data_en\")\n",
    "\n",
    "interactions = (\n",
    "    pd.read_csv(DATA_PATH / 'interactions.csv', parse_dates=[\"last_watch_dt\"])\n",
    "    .rename(columns={\"last_watch_dt\": \"datetime\"})\n",
    ")\n",
    "interactions[Columns.Weight] = 1\n",
    "dataset = Dataset.construct(\n",
    "    interactions_df=interactions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE=60\n",
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get custom trainer\n",
    "\n",
    "def get_debug_trainer() -> Trainer:\n",
    "    return Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        min_epochs=1,\n",
    "        max_epochs=1,\n",
    "        deterministic=True,\n",
    "        enable_model_summary=True,\n",
    "        enable_progress_bar=False,\n",
    "        limit_train_batches=2,  # limit train batches for quick debug runs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Next Action\" training objective from Pinnerformer\n",
    "[PinnerFormer: Sequence Modeling for User Representation at Pinterest](https://arxiv.org/pdf/2205.04507)\n",
    "\n",
    "This training objective aims to predict the most recent action for each user. Thus only one target should be taken from each user sequence.\n",
    "\n",
    "We will take BERT4RecModel as our base class and just change one single detail in data preparation: let's put \"MASK\" token replacing the last position of each user sequence. Everything else will work out of the box.\n",
    "\n",
    "For computational efficiency we will return `y` and `yw` (and `negatives`) in the shape of `(batch_size, 1)` instead of `(batch_size, session_max_len)`.\n",
    "To process this reshaped batch correctly during training we will also rewrite training step in lightning module.\n",
    "\n",
    "We could have filled `y` and `yw` with zeros except for the last target item. This way trainig step should have been left unchanged. But it's less efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data preparator and lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextActionDataPreparator(BERT4RecDataPreparator):\n",
    "    \n",
    "    def _collate_fn_train(\n",
    "        self,\n",
    "        batch: tp.List[tp.Tuple[tp.List[int], tp.List[float]]],\n",
    "    ) -> tp.Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Truncate each session from right to keep `session_max_len` items.\n",
    "        Do left padding until `session_max_len` is reached.\n",
    "        Split to `x`, `y`, and `yw`.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        x = np.zeros((batch_size, self.session_max_len))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        yw = np.zeros((batch_size, 1))\n",
    "        for i, (ses, ses_weights) in enumerate(batch):\n",
    "            session = ses.copy()\n",
    "            session[-1] = self.extra_token_ids[MASKING_VALUE]  # Replace last token with \"MASK\"\n",
    "            x[i, -len(ses) :] = session\n",
    "            y[i] = ses[-1]\n",
    "            yw[i] = ses_weights[-1]\n",
    "\n",
    "        batch_dict = {\"x\": torch.LongTensor(x), \"y\": torch.LongTensor(y), \"yw\": torch.FloatTensor(yw)}\n",
    "        if self.n_negatives is not None:\n",
    "            negatives = torch.randint(\n",
    "                low=self.n_item_extra_tokens,\n",
    "                high=self.item_id_map.size,\n",
    "                size=(batch_size, 1, self.n_negatives),\n",
    "            )\n",
    "            batch_dict[\"negatives\"] = negatives\n",
    "        return batch_dict\n",
    "\n",
    "\n",
    "class NextActionLightningModule(TransformerLightningModule):\n",
    "\n",
    "    def training_step(self, batch: tp.Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        x, y, w = batch[\"x\"], batch[\"y\"], batch[\"yw\"]\n",
    "        if self.loss == \"softmax\":\n",
    "            logits = self._get_full_catalog_logits(x)[:, -1: :]  # take only token last hidden state\n",
    "            loss = self._calc_softmax_loss(logits, y, w)\n",
    "        elif self.loss == \"BCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]  # take only last token hidden state\n",
    "            loss = self._calc_bce_loss(logits, y, w)\n",
    "        elif self.loss == \"gBCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)[:, -1: :]  # take only last token hidden state\n",
    "            loss = self._calc_gbce_loss(logits, y, w, negatives)\n",
    "        else:\n",
    "            loss = self._calc_custom_loss(batch, batch_idx)\n",
    "\n",
    "        self.log(self.train_loss_name, loss, on_step=False, on_epoch=True, prog_bar=self.verbose > 0)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `NextActionTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 5.5 M  | train\n",
      "-----------------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "22.040    Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.transformers.bert4rec.BERT4RecModel at 0x7fe59b52e3a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action_model = BERT4RecModel(\n",
    "    data_preparator_type=NextActionDataPreparator,\n",
    "    lightning_module_type=NextActionLightningModule,\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")\n",
    "\n",
    "next_action_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable unidirectional attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 5.5 M  | train\n",
      "-----------------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "22.040    Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.transformers.bert4rec.BERT4RecModel at 0x7fe59b52e3a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action_model_causal = BERT4RecModel(\n",
    "    data_preparator_type=NextActionDataPreparator,\n",
    "    lightning_module_type=NextActionLightningModule,\n",
    "    get_trainer_func = get_debug_trainer,\n",
    "    use_causal_attn = True,  # simple flag\n",
    ")\n",
    "next_action_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT\n",
    "[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)\n",
    "\n",
    "ALBERT has two parameter-reduction techniques to lower memory consumption and increase the training speed which can actually be used together or separately:\n",
    "1. Learning embeddings of smaller size and then projecting them to the required size through a Liner projection (\"Factorized embedding parameterization\")\n",
    "2. Sharing weights between transformer layers (\"Cross-layer parameter sharing\")\n",
    "\n",
    "We will implement both techiques in custom classes for transformer layers and for item net constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom item net constructor and transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special ALBERT logic for embeddings - Factorized embedding parameterization\n",
    "\n",
    "class AlbertSumConstructor(SumOfEmbeddingsConstructor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_items: int,\n",
    "        n_factors: int,\n",
    "        item_net_blocks: tp.Sequence[ItemNetBase],\n",
    "        emb_factors: int = 16,  # accept new kwarg for lower dimensional space size\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            n_items=n_items,\n",
    "            item_net_blocks=item_net_blocks,\n",
    "        )\n",
    "        self.item_emb_proj = nn.Linear(emb_factors, n_factors)  # Project to actual required hidden space\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: Dataset,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "        emb_factors: int,  # accept new kwarg for lower dimensional space size\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset.item_id_map.size\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            # Item net blocks will work in lower dimensional space\n",
    "            item_net_block = item_net.from_dataset(dataset, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, n_factors, item_net_blocks, emb_factors)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_schema(\n",
    "        cls,\n",
    "        dataset_schema: DatasetSchema,\n",
    "        n_factors: int,\n",
    "        dropout_rate: float,\n",
    "        item_net_block_types: tp.Sequence[tp.Type[ItemNetBase]],\n",
    "        emb_factors: int,  # accept new kwarg for lower dimensional space size\n",
    "    ) -> tpe.Self:\n",
    "        n_items = dataset_schema.items.n_hot\n",
    "\n",
    "        item_net_blocks: tp.List[ItemNetBase] = []\n",
    "        for item_net in item_net_block_types:\n",
    "            item_net_block = item_net.from_dataset_schema(dataset_schema, emb_factors, dropout_rate)\n",
    "            if item_net_block is not None:\n",
    "                item_net_blocks.append(item_net_block)\n",
    "\n",
    "        return cls(n_items, n_factors, item_net_blocks, emb_factors)\n",
    "\n",
    "    def forward(self, items: torch.Tensor) -> torch.Tensor:\n",
    "        item_embs = super().forward(items)  # Create embeddings in lower dimensional space\n",
    "        item_embs = self.item_emb_proj(item_embs)  # Project to actual required hidden space\n",
    "        return item_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special ALBERT logic for transformer layers - Cross-layer parameter sharing\n",
    "    \n",
    "class AlbertLayers(TransformerLayersBase):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks: int,\n",
    "        n_factors: int,\n",
    "        n_heads: int,\n",
    "        dropout_rate: float,\n",
    "        ff_factors_multiplier: int = 4,\n",
    "        n_hidden_groups: int=1,  # accept new kwarg\n",
    "        n_inner_groups: int=1,  # accept new kwarg\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_hidden_groups = n_hidden_groups\n",
    "        self.n_inner_groups = n_inner_groups\n",
    "        n_fitted_blocks = int(n_hidden_groups * n_inner_groups)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                PreLNTransformerLayer(\n",
    "                    # number of encoder layer (AlBERTLayers)\n",
    "                    # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L428\n",
    "                    n_factors,\n",
    "                    n_heads,\n",
    "                    dropout_rate,\n",
    "                    ff_factors_multiplier,\n",
    "                )\n",
    "                # https://github.com/huggingface/transformers/blob/main/src/transformers/models/albert/modeling_albert.py#L469\n",
    "                for _ in range(n_fitted_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.n_layers_per_group = n_blocks / n_hidden_groups\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        seqs: torch.Tensor,\n",
    "        timeline_mask: torch.Tensor,\n",
    "        attn_mask: tp.Optional[torch.Tensor],\n",
    "        key_padding_mask: tp.Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        for block_idx in range(self.n_blocks):\n",
    "            group_idx = int(block_idx / self.n_layers_per_group)\n",
    "            for inner_layer_idx in range(self.n_inner_groups):\n",
    "                layer_idx = group_idx * self.n_inner_groups + inner_layer_idx\n",
    "                seqs = self.transformer_blocks[block_idx](seqs, attn_mask, key_padding_mask)\n",
    "        return seqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass ALBERT modules to `BERT4RecModel`\n",
    "Now we need to pass both our custom classes and their keyword arguments for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.2 M  | train\n",
      "-----------------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.710    Total estimated model params size (MB)\n",
      "64        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.transformers.bert4rec.BERT4RecModel at 0x7fe42fd188e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONSTRUCTOR_KWARGS = {\n",
    "    \"emb_factors\": 64,\n",
    "}\n",
    "\n",
    "TRANSFORMER_LAYERS_KWARGS = {\n",
    "    \"n_hidden_groups\": 2,\n",
    "    \"n_inner_groups\": 2,\n",
    "}\n",
    "\n",
    "albert_model = BERT4RecModel(\n",
    "    item_net_constructor_type=AlbertSumConstructor,      # type\n",
    "    item_net_constructor_kwargs=CONSTRUCTOR_KWARGS,      # kwargs\n",
    "    transformer_layers_type=AlbertLayers,                # type\n",
    "    transformer_layers_kwargs=TRANSFORMER_LAYERS_KWARGS, # kwargs\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")\n",
    "\n",
    "albert_model.fit(dataset)\n",
    "# See that with Albert modules we have 4.2 M trainable params instead of 5.5 M previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass ALBERT modules to `SASRecModel`\n",
    "We are not limited to BERT4Rec when we just changed embedding and transformer layers logic.\n",
    "Why not create ALSASRec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.2 M  | train\n",
      "-----------------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.711    Total estimated model params size (MB)\n",
      "64        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.transformers.sasrec.SASRecModel at 0x7fe4c3453df0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alsasrec_model = SASRecModel(\n",
    "    item_net_constructor_type=AlbertSumConstructor,\n",
    "    item_net_constructor_kwargs=CONSTRUCTOR_KWARGS,\n",
    "    transformer_layers_type=AlbertLayers,\n",
    "    transformer_layers_kwargs=TRANSFORMER_LAYERS_KWARGS,\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")\n",
    "alsasrec_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about `NextActionTransformer` with ALBERT modules and causal attention?\n",
    "Just because we can!\n",
    "### Combining custom modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.2 M  | train\n",
      "-----------------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.710    Total estimated model params size (MB)\n",
      "64        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.transformers.bert4rec.BERT4RecModel at 0x7fe3cacf43d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action_albert_causal = BERT4RecModel(\n",
    "    item_net_constructor_type=AlbertSumConstructor,\n",
    "    item_net_constructor_kwargs=CONSTRUCTOR_KWARGS,\n",
    "    transformer_layers_type=AlbertLayers,\n",
    "    transformer_layers_kwargs=TRANSFORMER_LAYERS_KWARGS,\n",
    "    data_preparator_type=NextActionDataPreparator,\n",
    "    lightning_module_type=NextActionLightningModule,\n",
    "    use_causal_attn=True,\n",
    "    get_trainer_func = get_debug_trainer,\n",
    ")\n",
    "next_action_albert_causal.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Let's validate our custom models compared to vanilla SASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Initialize models for cross-validation\n",
    "\n",
    "def get_trainer() -> Trainer:\n",
    "    return Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[1],\n",
    "        min_epochs=3,\n",
    "        max_epochs=3,\n",
    "        deterministic=True,\n",
    "        enable_model_summary=False,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "next_action_bidirectional = BERT4RecModel(\n",
    "    data_preparator_type=NextActionDataPreparator,\n",
    "    lightning_module_type=NextActionLightningModule,\n",
    "    deterministic=True,\n",
    "    get_trainer_func=get_trainer,\n",
    ")\n",
    "\n",
    "next_action_unidirectional = BERT4RecModel(\n",
    "    data_preparator_type=NextActionDataPreparator,\n",
    "    lightning_module_type=NextActionLightningModule,\n",
    "    deterministic=True,\n",
    "    use_causal_attn=True,\n",
    "    get_trainer_func=get_trainer,\n",
    ")\n",
    "\n",
    "CONSTRUCTOR_KWARGS = {\n",
    "    \"emb_factors\": 64,\n",
    "}\n",
    "TRANSFORMER_LAYERS_KWARGS = {\n",
    "    \"n_hidden_groups\": 2,\n",
    "    \"n_inner_groups\": 2,\n",
    "}\n",
    "\n",
    "albert = BERT4RecModel(\n",
    "    item_net_constructor_type=AlbertSumConstructor,\n",
    "    item_net_constructor_kwargs=CONSTRUCTOR_KWARGS,\n",
    "    transformer_layers_type=AlbertLayers,\n",
    "    transformer_layers_kwargs=TRANSFORMER_LAYERS_KWARGS,\n",
    "    deterministic=True,\n",
    "    get_trainer_func=get_trainer,\n",
    ")\n",
    "\n",
    "alsasrec = SASRecModel(\n",
    "    item_net_constructor_type=AlbertSumConstructor,\n",
    "    item_net_constructor_kwargs=CONSTRUCTOR_KWARGS,\n",
    "    transformer_layers_type=AlbertLayers,\n",
    "    transformer_layers_kwargs=TRANSFORMER_LAYERS_KWARGS,\n",
    "    deterministic=True,\n",
    "    get_trainer_func=get_trainer,\n",
    ")\n",
    "\n",
    "sasrec_albert_layers = SASRecModel(\n",
    "    transformer_layers_type=AlbertLayers,\n",
    "    transformer_layers_kwargs=TRANSFORMER_LAYERS_KWARGS,\n",
    "    deterministic=True,\n",
    "    get_trainer_func=get_trainer,\n",
    ")\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"popular\": PopularModel(),\n",
    "    \"sasrec\":  SASRecModel(deterministic=True),\n",
    "    \"next_action_bidirectional\": next_action_bidirectional,\n",
    "    \"next_action_unidirectional\": next_action_unidirectional,\n",
    "    \"albert\": albert,\n",
    "    \"alsasrec\": alsasrec,\n",
    "    \"sasrec_albert_layers\": sasrec_albert_layers,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "# Validate models\n",
    "\n",
    "metrics = {\n",
    "    \"HitRate@10\": HitRate(k=10),\n",
    "    \"MAP@10\": MAP(k=10),\n",
    "    \"Serendipity@10\": Serendipity(k=10),\n",
    "    \"CoveredUsers@10\": CoveredUsers(k=10),  # how many test users received recommendations\n",
    "    \"AvgRecPopularity@10\": AvgRecPopularity(k=10),  # average popularity of recommended items\n",
    "    \"Intersection@10\": Intersection(k=10),  # intersection with recommendations from reference model\n",
    "}\n",
    "\n",
    "splitter = TimeRangeSplitter(\n",
    "    test_size=\"7D\",\n",
    "    n_splits=1,  # 1 fold\n",
    "    filter_already_seen=True,\n",
    ")\n",
    "\n",
    "K_RECS = 10\n",
    "\n",
    "# For each fold generate train and test part of dataset\n",
    "# Then fit every model, generate recommendations and calculate metrics\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    dataset=dataset,\n",
    "    splitter=splitter,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    k=K_RECS,\n",
    "    filter_viewed=True,\n",
    "    ref_models=[\"popular\"],  # pass reference model to calculate recommendations intersection\n",
    "    validate_ref_models=True,\n",
    ")\n",
    "\n",
    "pivot_results = (\n",
    "    pd.DataFrame(cv_results[\"metrics\"])\n",
    "    .drop(columns=\"i_split\")\n",
    "    .groupby([\"model\"], sort=False)\n",
    "    .agg([\"mean\"])\n",
    ")\n",
    "pivot_results.columns = pivot_results.columns.droplevel(1)\n",
    "pivot_results.to_csv(\"rectools_custom_transformers_cv_en.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HitRate@10</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>AvgRecPopularity@10</th>\n",
       "      <th>Serendipity@10</th>\n",
       "      <th>Intersection@10_popular</th>\n",
       "      <th>CoveredUsers@10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>popular</th>\n",
       "      <td>0.274365</td>\n",
       "      <td>0.080114</td>\n",
       "      <td>82236.761783</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasrec</th>\n",
       "      <td>0.316917</td>\n",
       "      <td>0.092236</td>\n",
       "      <td>70526.243531</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.621130</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_action_bidirectional</th>\n",
       "      <td>0.347769</td>\n",
       "      <td>0.099488</td>\n",
       "      <td>57260.700878</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.461527</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_action_unidirectional</th>\n",
       "      <td>0.342954</td>\n",
       "      <td>0.100581</td>\n",
       "      <td>54372.509136</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.445071</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert</th>\n",
       "      <td>0.332552</td>\n",
       "      <td>0.095702</td>\n",
       "      <td>62428.868590</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.514052</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alsasrec</th>\n",
       "      <td>0.346951</td>\n",
       "      <td>0.098554</td>\n",
       "      <td>50137.404580</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.393441</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasrec_albert_layers</th>\n",
       "      <td>0.347487</td>\n",
       "      <td>0.100079</td>\n",
       "      <td>50387.782216</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.424036</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            HitRate@10    MAP@10  AvgRecPopularity@10  \\\n",
       "model                                                                   \n",
       "popular                       0.274365  0.080114         82236.761783   \n",
       "sasrec                        0.316917  0.092236         70526.243531   \n",
       "next_action_bidirectional     0.347769  0.099488         57260.700878   \n",
       "next_action_unidirectional    0.342954  0.100581         54372.509136   \n",
       "albert                        0.332552  0.095702         62428.868590   \n",
       "alsasrec                      0.346951  0.098554         50137.404580   \n",
       "sasrec_albert_layers          0.347487  0.100079         50387.782216   \n",
       "\n",
       "                            Serendipity@10  Intersection@10_popular  \\\n",
       "model                                                                 \n",
       "popular                           0.000002                 1.000000   \n",
       "sasrec                            0.000029                 0.621130   \n",
       "next_action_bidirectional         0.000099                 0.461527   \n",
       "next_action_unidirectional        0.000107                 0.445071   \n",
       "albert                            0.000050                 0.514052   \n",
       "alsasrec                          0.000199                 0.393441   \n",
       "sasrec_albert_layers              0.000250                 0.424036   \n",
       "\n",
       "                            CoveredUsers@10  \n",
       "model                                        \n",
       "popular                                 1.0  \n",
       "sasrec                                  1.0  \n",
       "next_action_bidirectional               1.0  \n",
       "next_action_unidirectional              1.0  \n",
       "albert                                  1.0  \n",
       "alsasrec                                1.0  \n",
       "sasrec_albert_layers                    1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs support for custom models\n",
    "All custom models fully support initialization from configs and other RecTools benefits. For models with keyword arguments we suggest to use `from_params` method that accepts configs in a flat dict form. See example below:\n",
    "\n",
    "**Important: only JSON serializable custom keyword argument values are accepted during customization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls': 'BERT4RecModel',\n",
       " 'verbose': 0,\n",
       " 'data_preparator_type': '__main__.NextActionDataPreparator',\n",
       " 'n_blocks': 2,\n",
       " 'n_heads': 4,\n",
       " 'n_factors': 256,\n",
       " 'use_pos_emb': True,\n",
       " 'use_causal_attn': True,\n",
       " 'use_key_padding_mask': True,\n",
       " 'dropout_rate': 0.2,\n",
       " 'session_max_len': 100,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'batch_size': 128,\n",
       " 'loss': 'softmax',\n",
       " 'n_negatives': 1,\n",
       " 'gbce_t': 0.2,\n",
       " 'lr': 0.001,\n",
       " 'epochs': 3,\n",
       " 'deterministic': False,\n",
       " 'recommend_batch_size': 256,\n",
       " 'recommend_device': None,\n",
       " 'recommend_n_threads': 0,\n",
       " 'recommend_use_torch_ranking': True,\n",
       " 'train_min_user_interactions': 2,\n",
       " 'item_net_block_types': ['rectools.models.nn.item_net.IdEmbeddingsItemNet',\n",
       "  'rectools.models.nn.item_net.CatFeaturesItemNet'],\n",
       " 'item_net_constructor_type': '__main__.AlbertSumConstructor',\n",
       " 'pos_encoding_type': 'rectools.models.nn.transformers.net_blocks.LearnableInversePositionalEncoding',\n",
       " 'transformer_layers_type': '__main__.AlbertLayers',\n",
       " 'lightning_module_type': '__main__.NextActionLightningModule',\n",
       " 'get_val_mask_func': None,\n",
       " 'get_trainer_func': '__main__.get_debug_trainer',\n",
       " 'data_preparator_kwargs': None,\n",
       " 'transformer_layers_kwargs.n_hidden_groups': 2,\n",
       " 'transformer_layers_kwargs.n_inner_groups': 2,\n",
       " 'item_net_constructor_kwargs.emb_factors': 64,\n",
       " 'pos_encoding_kwargs': None,\n",
       " 'lightning_module_kwargs': None,\n",
       " 'mask_prob': 0.15}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = next_action_albert_causal.get_params(simple_types=True)\n",
    "params\n",
    "# See below that model params include our custom keyword arguments:\n",
    "# \"transformer_layers_kwargs.n_hidden_groups\", \n",
    "# \"transformer_layers_kwargs.n_inner_groups\"\n",
    "# and \"item_net_constructor_kwargs.emb_factors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 4.2 M  | train\n",
      "-----------------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.710    Total estimated model params size (MB)\n",
      "64        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rectools.models.nn.transformers.bert4rec.BERT4RecModel at 0x7fe30bdb9ee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT4RecModel.from_params(params)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full list of customization options\n",
    "\n",
    "These blocks of RecTools transformer models can be replaced with custom classes (WITH an option to add required keyword arguments for initialization):\n",
    "- data preparator (`data_preparator_type`, `data_preparator_kwargs`)\n",
    "    - forming training objectives\n",
    "    - providing train, val and recommend dataloaders preparation\n",
    "- lightning module (`lightning_module_type`, `lightning_module_kwargs`)\n",
    "    - tying of user session latent represenation and candidate embeddings\n",
    "    - training, validation and recommending logic\n",
    "    - losses computation\n",
    "    - weights initialization\n",
    "    - optimizer configuration\n",
    "- item net constructor (`item_net_constructor_type`, `item_net_constructor_kwargs`)\n",
    "    - way for aggregating outputs from item net blocks\n",
    "- transformer layers (`transformer_layers_type`, `transformer_layers_kwargs`)\n",
    "- positional encoding (`pos_encoding_type`, `pos_encoding_kwargs`)\n",
    "\n",
    "These blocks of RecTools transformer models can be replaced with custom classes (WITHOUT an option to add keyword arguments):\n",
    "- item net blocks (`item_net_block_types`)\n",
    "\n",
    "These keyword model arguments have great effect on model architecture:\n",
    "- `use_causal_attn` (applies unidirectional attention instead of bidirectional when set to ``True``)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rectools",
   "language": "python",
   "name": "rectools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
