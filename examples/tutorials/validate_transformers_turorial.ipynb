{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: will remove\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import typing as tp\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from rectools import Columns\n",
    "from rectools import ExternalIds\n",
    "from rectools.dataset import Dataset, Interactions, IdMap\n",
    "from rectools.metrics import NDCG, Recall, Serendipity, calc_metrics\n",
    "\n",
    "from rectools.models.sasrec import (\n",
    "    SASRecModel,\n",
    "    SASRecDataPreparator,\n",
    "    SequenceDataset, \n",
    "    SessionEncoderLightningModule,\n",
    "    TransformerBasedSessionEncoder,\n",
    "    PADDING_VALUE,\n",
    "    IdEmbeddingsItemNet,\n",
    ")\n",
    "\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!wget -q https://github.com/irsafilo/KION_DATASET/raw/f69775be31fa5779907cf0a92ddedb70037fb5ae/data_en.zip -O data_en.zip\n",
    "!unzip -o data_en.zip\n",
    "!rm data_en.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476251, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>watched_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176549</td>\n",
       "      <td>9506</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>4250</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>699317</td>\n",
       "      <td>1659</td>\n",
       "      <td>2021-05-29</td>\n",
       "      <td>8317</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id   datetime  total_dur  watched_pct\n",
       "0   176549     9506 2021-05-11       4250         72.0\n",
       "1   699317     1659 2021-05-29       8317        100.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download dataset\n",
    "DATA_PATH = Path(\"data_en\")\n",
    "items = pd.read_csv(DATA_PATH / 'items_en.csv', index_col=0)\n",
    "interactions = (\n",
    "    pd.read_csv(DATA_PATH / 'interactions.csv', parse_dates=[\"last_watch_dt\"])\n",
    "    .rename(columns={\"last_watch_dt\": Columns.Datetime})\n",
    ")\n",
    "\n",
    "print(interactions.shape)\n",
    "interactions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962179, 15706)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions[Columns.User].nunique(), interactions[Columns.Item].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476251, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176549</td>\n",
       "      <td>9506</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>699317</td>\n",
       "      <td>1659</td>\n",
       "      <td>2021-05-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id   datetime  weight\n",
       "0   176549     9506 2021-05-11       3\n",
       "1   699317     1659 2021-05-29       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process interactions\n",
    "interactions[Columns.Weight] = np.where(interactions['watched_pct'] > 10, 3, 1)\n",
    "interactions = interactions[[\"user_id\", \"item_id\", \"datetime\", \"weight\"]]\n",
    "print(interactions.shape)\n",
    "interactions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process item features\n",
    "items = items.loc[items[Columns.Item].isin(interactions[Columns.Item])].copy()\n",
    "items[\"genre\"] = items[\"genres\"].str.lower().str.replace(\", \", \",\", regex=False).str.split(\",\")\n",
    "genre_feature = items[[\"item_id\", \"genre\"]].explode(\"genre\")\n",
    "genre_feature.columns = [\"id\", \"value\"]\n",
    "genre_feature[\"feature\"] = \"genre\"\n",
    "content_feature = items.reindex(columns=[Columns.Item, \"content_type\"])\n",
    "content_feature.columns = [\"id\", \"value\"]\n",
    "content_feature[\"feature\"] = \"content_type\"\n",
    "item_features = pd.concat((genre_feature, content_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE=60\n",
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(user_id_map=IdMap(external_ids=array([176549, 699317, 656683, ..., 805174, 648596, 697262])), item_id_map=IdMap(external_ids=array([ 9506,  1659,  7107, ..., 10064, 13019, 10542])), interactions=Interactions(df=         user_id  item_id  weight   datetime\n",
       "0              0        0     3.0 2021-05-11\n",
       "1              1        1     3.0 2021-05-29\n",
       "2              2        2     1.0 2021-05-09\n",
       "3              3        3     3.0 2021-07-05\n",
       "4              4        0     3.0 2021-04-30\n",
       "...          ...      ...     ...        ...\n",
       "5476246   962177      208     1.0 2021-08-13\n",
       "5476247   224686     2690     3.0 2021-04-13\n",
       "5476248   962178       21     3.0 2021-08-20\n",
       "5476249     7934     1725     3.0 2021-04-19\n",
       "5476250   631989      157     3.0 2021-08-15\n",
       "\n",
       "[5476251 rows x 4 columns]), user_features=None, item_features=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_no_features = Dataset.construct(interactions)\n",
    "dataset_no_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custome Validation** (Leave-One-Out Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recos_df_from_logits(\n",
    "    logits: torch.Tensor, candidates: torch.Tensor, user_ids: tp.List[int], top_k: int\n",
    ") -> pd.DataFrame:\n",
    "    _, indexes = logits.topk(k=top_k)\n",
    "    sorted_recos = candidates.gather(1, indexes).tolist()\n",
    "\n",
    "    batch_recos = pd.DataFrame(\n",
    "        {\n",
    "            Columns.User: user_ids,\n",
    "            Columns.Item: sorted_recos\n",
    "        }\n",
    "    ).explode(column=Columns.Item)\n",
    "    batch_recos[Columns.Rank] = batch_recos.groupby(Columns.User, sort=False).cumcount() + 1\n",
    "\n",
    "    return batch_recos\n",
    "\n",
    "\n",
    "class SequenceDatasetValidation(SequenceDataset):\n",
    "    \n",
    "    def __init__(self, sessions: List[List[int]], weights: List[List[float]], user_ids: List[int]):\n",
    "        super().__init__(sessions=sessions, weights=weights)\n",
    "        self.user_ids = user_ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[List[int], List[float], List[int]]:\n",
    "        session, weights = super().__getitem__(index=index)\n",
    "        user_ids = self.user_ids[index]\n",
    "        return session, weights, user_ids\n",
    "\n",
    "    @classmethod\n",
    "    def from_interactions(\n",
    "        cls,\n",
    "        interactions: pd.DataFrame,\n",
    "    ) -> \"SequenceDatasetValidation\":\n",
    "        \"\"\"\n",
    "        Group interactions by user.\n",
    "        Construct SequenceDataset from grouped interactions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        interactions: pd.DataFrame\n",
    "            User-item interactions.\n",
    "        \"\"\"\n",
    "        sessions = (\n",
    "            interactions.sort_values(Columns.Datetime)\n",
    "            .groupby(Columns.User, sort=True, as_index=False)[[Columns.Item, Columns.Weight]]\n",
    "            .agg(list)\n",
    "        )\n",
    "        user_ids, sessions, weights = (\n",
    "            sessions[Columns.User].to_list(),\n",
    "            sessions[Columns.Item].to_list(),\n",
    "            sessions[Columns.Weight].to_list(),\n",
    "        )\n",
    "\n",
    "        return cls(sessions=sessions, weights=weights, user_ids=user_ids)\n",
    "\n",
    "\n",
    "class SASRecDataPreparatorValidate(SASRecDataPreparator):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        session_max_len: int,\n",
    "        batch_size: int,\n",
    "        dataloader_num_workers: int,\n",
    "        shuffle_train: bool = True,\n",
    "        item_extra_tokens: tp.Sequence[tp.Hashable] = (PADDING_VALUE,),\n",
    "        train_min_user_interactions: int = 2,\n",
    "        n_negatives: tp.Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            session_max_len=session_max_len,\n",
    "            batch_size=batch_size,\n",
    "            dataloader_num_workers=dataloader_num_workers,\n",
    "            shuffle_train=shuffle_train,\n",
    "            item_extra_tokens=item_extra_tokens,\n",
    "            train_min_user_interactions=train_min_user_interactions,\n",
    "            n_negatives=n_negatives,\n",
    "        )\n",
    "        \n",
    "        self.val_k_out: int\n",
    "        self.num_users_interacted_with_item: pd.DataFrame\n",
    "\n",
    "    def process_dataset_train(self, dataset: Dataset, val_users: ExternalIds, val_k_out: int) -> tp.Dict[str, Dataset]:\n",
    "        self.val_k_out = val_k_out\n",
    "\n",
    "        interactions = dataset.get_raw_interactions()\n",
    "\n",
    "        interactions_train = interactions.copy()\n",
    "        interactions_val = interactions.copy()\n",
    "\n",
    "        ### Ctreating train dataset\n",
    "        interactions_train[f\"{Columns.Rank}_inverse\"] = (\n",
    "            interactions_train.sort_values(Columns.Datetime, ascending=False)\n",
    "            .groupby(Columns.User)\n",
    "            .cumcount() + 1\n",
    "        )\n",
    "        mask_train = ~(\n",
    "            (interactions_train[Columns.User].isin(val_users))\n",
    "            & (interactions_train[f\"{Columns.Rank}_inverse\"].isin(range(1, self.val_k_out + 1)))\n",
    "        )\n",
    "        interactions_train.drop(columns=f\"{Columns.Rank}_inverse\", inplace=True)\n",
    "\n",
    "        interactions_train = interactions_train[mask_train]\n",
    "\n",
    "        user_id_map = IdMap.from_values(interactions_train[Columns.User].values)\n",
    "        item_id_map = IdMap.from_values(interactions_train[Columns.Item].values)\n",
    "        item_features = None\n",
    "        if dataset.item_features is not None:\n",
    "            item_features = dataset.item_features.take(item_id_map.internal_ids)\n",
    "\n",
    "        interactions_train = Interactions.from_raw(\n",
    "            interactions_train, user_id_map,  item_id_map, keep_extra_cols=False\n",
    "        )\n",
    "        dataset_train = Dataset(user_id_map, dataset.item_id_map, interactions_train, item_features=item_features)\n",
    "        processed_dataset_train = super().process_dataset_train(dataset_train)\n",
    "        \n",
    "        ### Ctreating validation dataset\n",
    "        interactions_val = (\n",
    "            interactions_val[\n",
    "                (interactions_val[Columns.User].isin(val_users))\n",
    "                & (interactions_val[Columns.User].isin(processed_dataset_train.user_id_map.to_external))\n",
    "                & (interactions_val[Columns.Item].isin(processed_dataset_train.item_id_map.to_external))\n",
    "            ]\n",
    "        )\n",
    "        interactions_val[f\"{Columns.Rank}_inverse\"] = (\n",
    "            interactions_val.sort_values(Columns.Datetime, ascending=False)\n",
    "            .groupby(Columns.User)\n",
    "            .cumcount() + 1\n",
    "        )\n",
    "        mask_val = interactions_val[f\"{Columns.Rank}_inverse\"].isin(range(1, self.val_k_out + 1))\n",
    "        interactions_val.drop(columns=f\"{Columns.Rank}_inverse\", inplace=True)\n",
    "\n",
    "        interactions_val.loc[~mask_val, Columns.Weight] = 0\n",
    "        interactions_val = interactions_val.sort_values(Columns.Datetime).groupby(Columns.User).tail(self.session_max_len + self.val_k_out)\n",
    "        interactions_val = Interactions.from_raw(\n",
    "            interactions_val, \n",
    "            processed_dataset_train.user_id_map, \n",
    "            processed_dataset_train.item_id_map, \n",
    "            keep_extra_cols=False,\n",
    "        )\n",
    "        processed_dataset_val = Dataset(\n",
    "            processed_dataset_train.user_id_map, \n",
    "            processed_dataset_train.item_id_map, \n",
    "            interactions_val,\n",
    "            item_features=processed_dataset_train.item_features,\n",
    "        )\n",
    "\n",
    "        num_users_interacted_with_item = (\n",
    "            processed_dataset_train.interactions.df.groupby(Columns.Item, sort=False, as_index=False)[\n",
    "                Columns.User\n",
    "            ].nunique()\n",
    "            .rename(columns={Columns.User: \"popularity\"})\n",
    "        )\n",
    "        num_users_interacted_with_item = num_users_interacted_with_item[\n",
    "            ~num_users_interacted_with_item[Columns.Item].isin(range(self.n_item_extra_tokens))\n",
    "        ].reset_index(drop=True)\n",
    "        num_users_interacted_with_item[\"popularity\"] /= num_users_interacted_with_item[\"popularity\"].sum()\n",
    "        \n",
    "        self.num_users_interacted_with_item = num_users_interacted_with_item\n",
    "        return {\"train\": processed_dataset_train, \"val\": processed_dataset_val}\n",
    "    \n",
    "    def get_dataloader_val(self, processed_dataset: tp.Optional[Dataset], val_neg_candidates: int) -> tp.Optional[DataLoader]:\n",
    "        sequence_dataset = SequenceDatasetValidation.from_interactions(processed_dataset.interactions.df)\n",
    "        train_dataloader = DataLoader(\n",
    "            sequence_dataset,\n",
    "            collate_fn=lambda batch: self._collate_fn_val(batch, val_neg_candidates),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.dataloader_num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return train_dataloader\n",
    "\n",
    "    def _collate_fn_val(self, batch: List[Tuple[List[int], List[float]]], val_neg_candidates: int) -> Dict[str, Tensor]:\n",
    "        batch_size = len(batch)\n",
    "        x = np.zeros((batch_size, self.session_max_len))\n",
    "        y = np.zeros((batch_size, self.val_k_out))\n",
    "        yw = np.zeros((batch_size, self.val_k_out))\n",
    "        user_ids = np.zeros((batch_size, self.val_k_out))\n",
    "        # uniformly_neg_candidates = np.zeros((batch_size, val_neg_candidates))\n",
    "        popularity_neg_candidates = np.zeros((batch_size, val_neg_candidates))\n",
    "\n",
    "        for i, (ses, ses_weights, user_id) in enumerate(batch):\n",
    "            input_session = [ses[idx] for idx, weight in enumerate(ses_weights) if weight == 0]\n",
    "            \n",
    "            target_idx = [idx for idx, weight in enumerate(ses_weights) if weight != 0]\n",
    "\n",
    "            targets = list(map(ses.__getitem__, target_idx))\n",
    "            targets_weights = list(map(ses_weights.__getitem__, target_idx))\n",
    "            user = [user_id for _ in range(len(target_idx))]\n",
    "\n",
    "            x[i, -len(input_session) :] = input_session[-self.session_max_len :]  # ses: [session_len] -> x[i]: [session_max_len]\n",
    "            y[i, :] = targets # y[i]: [val_k_out]\n",
    "            yw[i, :] = targets_weights  # yw[i]: [val_k_out]\n",
    "            user_ids[i, :] = user  # u_ids[i]: [val_k_out]\n",
    "            \n",
    "            # # Uniformly Sampled\n",
    "            # low_neg_item_id = self.n_item_extra_tokens\n",
    "            # high_neg_item_id = self.item_id_map.size\n",
    "            # neg = torch.randperm(high_neg_item_id - low_neg_item_id) + low_neg_item_id  # [self.item_id_map.size - self.n_item_extra_tokens]\n",
    "            # uniformly_neg_candidates[i, :] = [n for n in neg.tolist() if n not in targets][: val_neg_candidates]\n",
    "\n",
    "            # Popular Sampled\n",
    "            index_neg = torch.multinomial(\n",
    "                torch.tensor(self.num_users_interacted_with_item[\"popularity\"].tolist()), \n",
    "                num_samples=val_neg_candidates + self.val_k_out,\n",
    "                replacement=False\n",
    "            )\n",
    "            neg = self.num_users_interacted_with_item[Columns.Item].iloc[index_neg].tolist()\n",
    "            popularity_neg_candidates[i, :] = [n for n in neg if n not in targets][: val_neg_candidates]\n",
    "\n",
    "        batch_dict = {\n",
    "            \"x\": torch.LongTensor(x), \n",
    "            \"y\": torch.LongTensor(y), \n",
    "            \"yw\": torch.FloatTensor(yw),\n",
    "            \"user_ids\": torch.LongTensor(user_ids),\n",
    "            # \"uniformly_neg_candidates\": torch.LongTensor(uniformly_neg_candidates),\n",
    "            \"popularity_neg_candidates\": torch.LongTensor(popularity_neg_candidates),\n",
    "        }\n",
    "        # TODO: we are sampling negatives for paddings\n",
    "        if self.n_negatives is not None:\n",
    "            negatives = torch.randint(\n",
    "                low=self.n_item_extra_tokens,\n",
    "                high=self.item_id_map.size,\n",
    "                size=(batch_size, self.val_k_out, self.n_negatives),\n",
    "            )  # [batch_size, session_max_len, n_negatives]\n",
    "            batch_dict[\"negatives\"] = negatives\n",
    "        return batch_dict\n",
    "\n",
    "\n",
    "class SessionEncoderLightningModuleValidate(SessionEncoderLightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        torch_model: TransformerBasedSessionEncoder,\n",
    "        lr: float,\n",
    "        gbce_t: float,\n",
    "        n_item_extra_tokens: int,\n",
    "        val_metrics: tp.Dict,\n",
    "        val_dataset: Dataset,\n",
    "        loss: str = \"softmax\",\n",
    "        adam_betas: Tuple[float, float] = (0.9, 0.98),\n",
    "        val_k_out: int = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            torch_model=torch_model,\n",
    "            lr=lr,\n",
    "            gbce_t=gbce_t,\n",
    "            n_item_extra_tokens=n_item_extra_tokens,\n",
    "            loss=loss,\n",
    "            adam_betas=adam_betas\n",
    "        )        \n",
    "        \n",
    "        interactions = val_dataset.interactions.df\n",
    "\n",
    "        self.val_dataset = val_dataset\n",
    "        self.interactions = interactions[interactions[Columns.Weight] != 0]\n",
    "        self.prev_interactions = interactions[interactions[Columns.Weight] == 0]\n",
    "        self.catalog = val_dataset.item_id_map.to_internal\n",
    "        \n",
    "        self.val_metrics = val_metrics\n",
    "        self.metric_max_k = max([metric.k for metric in val_metrics.values()])\n",
    "\n",
    "        self.validation_step_recos_sampled = []\n",
    "        self.validation_step_recos_unsampled = []\n",
    "        self.val_metrics_result = []\n",
    "        self.val_k_out = val_k_out\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        train_loss = super().training_step(batch, batch_idx)\n",
    "        self.log(\"train/loss\", train_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        x, y, w = batch[\"x\"], batch[\"y\"], batch[\"yw\"]\n",
    "        sampled_neg_candidates = batch[\"popularity_neg_candidates\"]\n",
    "\n",
    "        user_ids = []\n",
    "        for uid in batch[\"user_ids\"].tolist():\n",
    "            user_ids.extend(uid)\n",
    "\n",
    "        last_logits_for_metrics = None\n",
    "        if self.loss == \"softmax\":\n",
    "            logits = self._get_full_catalog_logits(x)\n",
    "            last_logits_for_metrics = logits[:, -1:, :]\n",
    "            val_loss = self._calc_softmax_loss(last_logits_for_metrics, y, w)\n",
    "        elif self.loss == \"BCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)\n",
    "            last_logits = logits[:, -1:, :]\n",
    "            val_loss = self._calc_bce_loss(last_logits, y, w)\n",
    "        elif self.loss == \"gBCE\":\n",
    "            negatives = batch[\"negatives\"]\n",
    "            logits = self._get_pos_neg_logits(x, y, negatives)\n",
    "            last_logits = logits[:, -1:, :]\n",
    "            val_loss = self._calc_gbce_loss(last_logits, y, w, negatives)\n",
    "        else:\n",
    "            raise ValueError(f\"loss {self.loss} is not supported\")\n",
    "\n",
    "        if last_logits_for_metrics is not None:\n",
    "            last_logits_for_metrics = last_logits_for_metrics[:, -1, :].detach().cpu()\n",
    "\n",
    "            # Popularity sampled metrics\n",
    "            sampled_candidates = torch.concat([y, sampled_neg_candidates], dim=1).detach().cpu()\n",
    "\n",
    "            logits_candidates = last_logits_for_metrics.gather(1, sampled_candidates)\n",
    "            batch_recos_sampled = create_recos_df_from_logits(\n",
    "                logits=logits_candidates, \n",
    "                candidates=sampled_candidates, \n",
    "                user_ids=user_ids, \n",
    "                top_k=logits_candidates.shape[1]\n",
    "            )\n",
    "            \n",
    "            self.validation_step_recos_sampled.append(batch_recos_sampled)\n",
    "\n",
    "            # Unsampled metrics\n",
    "            unsampled_candidates = torch.arange(0, self.torch_model.item_model.n_items + 1)\n",
    "            batch_recos_unsampled = create_recos_df_from_logits(\n",
    "                logits=last_logits_for_metrics,\n",
    "                candidates=unsampled_candidates.repeat(last_logits_for_metrics.shape[0], 1), \n",
    "                user_ids=user_ids,\n",
    "                top_k=self.metric_max_k\n",
    "            )\n",
    "\n",
    "            self.validation_step_recos_unsampled.append(batch_recos_unsampled)\n",
    "\n",
    "        else:\n",
    "            warnings.warn(\"Can not caclulate RecSys metrics with `BCE` and `gBCE` losses\")\n",
    "\n",
    "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        if self.loss not in [\"BCE\", \"gBCE\"]:\n",
    "            sampled_recos = pd.concat(self.validation_step_recos_sampled, ignore_index=True)\n",
    "            unsampled_recos = pd.concat(self.validation_step_recos_unsampled, ignore_index=True)\n",
    "\n",
    "            sampled_metrics = calc_metrics(self.val_metrics, sampled_recos, self.interactions, self.prev_interactions, self.catalog)\n",
    "            unsampled_metrics = calc_metrics(self.val_metrics, unsampled_recos, self.interactions, self.prev_interactions, self.catalog)\n",
    "\n",
    "            sampled_metrics = {f\"{metric_name}_sampled\": metric_value for metric_name, metric_value in sampled_metrics.items()}\n",
    "            unsampled_metrics = {f\"{metric_name}_unsampled\": metric_value for metric_name, metric_value in unsampled_metrics.items()}\n",
    "\n",
    "            metrics = {**sampled_metrics, **unsampled_metrics}\n",
    "\n",
    "            self.log_dict(metrics)\n",
    "            self.validation_step_recos_sampled.clear()\n",
    "\n",
    "\n",
    "class SASRecModelValidateLeaveOneOut(SASRecModel):\n",
    "    val_k_out: int = 1\n",
    "\n",
    "    def _fit(self, dataset: Dataset, val_users: ExternalIds, val_metrics: tp.Dict, val_neg_candidates: int) -> None:\n",
    "        processed_datasets = self.data_preparator.process_dataset_train(dataset, val_users, self.val_k_out)\n",
    "        train_dataloader = self.data_preparator.get_dataloader_train(processed_datasets[\"train\"])\n",
    "        val_dataloader = self.data_preparator.get_dataloader_val(processed_datasets[\"val\"], val_neg_candidates)\n",
    "\n",
    "        torch_model = deepcopy(self._torch_model)  # TODO: check that it works\n",
    "        torch_model.construct_item_net(processed_datasets[\"train\"])\n",
    "\n",
    "        n_item_extra_tokens = self.data_preparator.n_item_extra_tokens\n",
    "        self.lightning_model = self.lightning_module_type(\n",
    "            torch_model=torch_model,\n",
    "            lr=self.lr,\n",
    "            loss=self.loss,\n",
    "            gbce_t=self.gbce_t,\n",
    "            n_item_extra_tokens=n_item_extra_tokens,\n",
    "            val_metrics=val_metrics,\n",
    "            val_dataset=processed_datasets[\"val\"],\n",
    "            val_k_out=self.val_k_out,\n",
    "        )\n",
    "        \n",
    "        self.fit_trainer = deepcopy(self._trainer)\n",
    "        self.fit_trainer.fit(self.lightning_model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir(trainer: Trainer) -> Path:\n",
    "    \"\"\"\n",
    "    Get logging directory.\n",
    "    \"\"\"\n",
    "    path = trainer.logger.log_dir\n",
    "    vesrion = int(path.split(\"version_\")[-1])\n",
    "    last_path = path.split(\"version_\")[0] + f\"version_{vesrion - 1}\"\n",
    "    return Path(last_path) / \"metrics.csv\"\n",
    "\n",
    "\n",
    "def get_losses(epoch_metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    train_loss_df = epoch_metrics_df[[\"epoch\", \"train/loss\"]].dropna()\n",
    "    val_loss_df = epoch_metrics_df[[\"epoch\", \"val/loss\"]].dropna()\n",
    "    loss_df = pd.merge(train_loss_df, val_loss_df, how=\"inner\", on=\"epoch\")\n",
    "    return loss_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_val_metrics(epoch_metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    metrics_df = epoch_metrics_df.drop(columns=[\"train/loss\", \"val/loss\"]).dropna()\n",
    "    return metrics_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_log_values(trainer: Trainer) -> tp.Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    log_path = get_log_dir(trainer)\n",
    "    epoch_metrics_df = pd.read_csv(log_path)\n",
    "\n",
    "    loss_df = get_losses(epoch_metrics_df)\n",
    "    val_metrics = get_val_metrics(epoch_metrics_df)\n",
    "    return loss_df, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((962179,), (2048,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_VAL_USERS = 2048\n",
    "VAL_NEG_CANDIDATES = 100\n",
    "\n",
    "unique_users = interactions[Columns.User].unique()\n",
    "VAL_USERS = unique_users[: N_VAL_USERS]\n",
    "unique_users.shape, VAL_USERS.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_METRICS = {\n",
    "    \"NDCG@10\": NDCG(k=10),\n",
    "    \"Recall@10\": Recall(k=10),\n",
    "    \"Serendipity@10\": Serendipity(k=10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 20\n",
    "MAX_EPOCHS = 100\n",
    "early_stopping = EarlyStopping(monitor=\"val/loss\", patience=PATIENCE, min_delta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=[0],\n",
    "    min_epochs=PATIENCE, \n",
    "    max_epochs=MAX_EPOCHS, \n",
    "    deterministic=True,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_non_default_model = SASRecModelValidateLeaveOneOut(\n",
    "    n_factors=64,\n",
    "    n_blocks=2,\n",
    "    n_heads=2,\n",
    "    dropout_rate=0.2,\n",
    "    use_pos_emb=True,\n",
    "    train_min_user_interaction=5,\n",
    "    session_max_len=50,\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    # epochs=5,\n",
    "    loss=\"softmax\",\n",
    "    verbose=1,\n",
    "    deterministic=True,\n",
    "    item_net_block_types=(IdEmbeddingsItemNet, ),  # Use only item ids in ItemNetBlock\n",
    "    data_preparator_type=SASRecDataPreparatorValidate, \n",
    "    lightning_module_type=SessionEncoderLightningModuleValidate,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type                           | Params\n",
      "---------------------------------------------------------------\n",
      "0 | torch_model | TransformerBasedSessionEncoder | 988 K \n",
      "---------------------------------------------------------------\n",
      "988 K     Trainable params\n",
      "0         Non-trainable params\n",
      "988 K     Total params\n",
      "3.952     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe1ca3c43ba4d95b37ccf2b56a558ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368b73ee9eda46af9330627b58a63668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5558bc70ef745e7bb0bb10083f3dece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a14ffc4a924e3d8330cf052798cfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37b270f96854059871dde718677b8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1972b8aead134099aac1d9c4d55da1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c026c1f76a2340a3a844280c89d2965b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b246ca9f114372a09f8dbb68560b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec70d1d2b5bb4f299992119077005879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0508317fded345c0a31be9cc628c3254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8d1d318244d81a5762911a80e9a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c1f4f260dd4769ad2a23ad83e3e098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51efa003ce64f0f906c756c21987af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8207717f91914322a37008b26662c7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42dd72780f04241a70a870debf3d9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec17c24db15f4403ac54f2dfbad1911e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e0bf6f92774fd6a01394e3a5950a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f065242659b742d4ae8e737678413cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635413b00d0642c6b27a4cb450da6d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29d1a0f280a421c90fd054abca197e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e9e2f9b0e3499eb6232903d590f686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e6ceb88aae4ba48f4e302c9f178b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b90b84401f044d6b8b750df843b65b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2677fe3a234648909991b7869262ad07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012bc68b1204433c985533725bce1d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c05d10287394c8384fc5d024ef79097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdefdc6dc7ca44f6abd0c56b3831c3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f048cd676d4b0b91927958b983ac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad3f60d35c04a7f9d039672dc2fc6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287419e18244e78aa9fff9be6865a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbab05ba9374995b584d244fedf9728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00eabdafce64299930cbae412ba3203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db750f3644b45df948ce60464e6ac96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4f3558fbc747159a14ed0bd11db867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121b3ccc0f3640a6bed230049126c62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5a42cce9414ca19810239efa101dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caf6cfb21ae420faf285d496407e561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e7784d108045f098dd5ebe8bb1b416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b11d21704e4e6ea26d2cd9089a259c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5662fb3a4084ff19b68d216c3b2dc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e6f662bbed44edbb06c44b51e9b97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68698ff1f84f4db327f192a03e78de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40026777e53149b7b4db1e976ebbdd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f52100e97a4a5a8fd3893da9c5cb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5912e5a64b486da945ed3d1d1cf69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826726e55e414c33b357a1b0fd7abd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16ea025e2fc4c8b9f3fcddaec73a99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa67a67b73e4edbb89718543ec9c45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbf586715994833b6f7a4ddf35d7611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff3dc51b297475b93fa8bf6c310a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa905a80f5da499f92f7b40a42d7f033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eadcce1366f4f78b4f0971f0f636e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb9969c592344d29f13811f45b22d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e292e08eaeba4db3a73ceb2765356f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4c3b50ca4647c888b235fb35a211f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b891ea47928d44aea3ed22e0f62864e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 26min 18s, sys: 1min 41s, total: 6h 28min\n",
      "Wall time: 1h 30min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SASRecModelValidateLeaveOneOut at 0x7f1d780cd220>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sasrec_non_default_model.fit(dataset_no_features, VAL_USERS, VAL_METRICS, VAL_NEG_CANDIDATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df, val_metrics_df = get_log_values(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>val/loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.388460</td>\n",
       "      <td>21.328363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.723532</td>\n",
       "      <td>21.301912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15.568251</td>\n",
       "      <td>21.398142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15.498148</td>\n",
       "      <td>21.538115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15.455770</td>\n",
       "      <td>21.400196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>15.430855</td>\n",
       "      <td>21.358656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15.408437</td>\n",
       "      <td>21.334433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15.392024</td>\n",
       "      <td>21.515078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>15.381497</td>\n",
       "      <td>21.288002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>15.367785</td>\n",
       "      <td>21.428801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>15.357292</td>\n",
       "      <td>21.377481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>15.348079</td>\n",
       "      <td>21.396805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>15.343961</td>\n",
       "      <td>21.282213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>15.336177</td>\n",
       "      <td>21.400455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>15.332963</td>\n",
       "      <td>21.392611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15.328608</td>\n",
       "      <td>21.469452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>15.325841</td>\n",
       "      <td>21.418718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>15.321985</td>\n",
       "      <td>21.345282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>15.321541</td>\n",
       "      <td>21.349298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>15.317352</td>\n",
       "      <td>21.325220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>15.315112</td>\n",
       "      <td>21.430372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>15.313273</td>\n",
       "      <td>21.325783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>15.311438</td>\n",
       "      <td>21.378805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>15.308382</td>\n",
       "      <td>21.441025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>15.307839</td>\n",
       "      <td>21.359484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>15.306304</td>\n",
       "      <td>21.492167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>15.307653</td>\n",
       "      <td>21.408672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>15.305036</td>\n",
       "      <td>21.348469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>15.305502</td>\n",
       "      <td>21.280390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>15.304422</td>\n",
       "      <td>21.386406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>15.304774</td>\n",
       "      <td>21.385309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>15.304351</td>\n",
       "      <td>21.343985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>15.302314</td>\n",
       "      <td>21.288002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>15.301056</td>\n",
       "      <td>21.275574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>15.300669</td>\n",
       "      <td>21.351576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>15.299047</td>\n",
       "      <td>21.414932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>15.298584</td>\n",
       "      <td>21.387619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>15.297479</td>\n",
       "      <td>21.325569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>15.293925</td>\n",
       "      <td>21.381674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>15.293931</td>\n",
       "      <td>21.451353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>15.292399</td>\n",
       "      <td>21.383957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>15.291520</td>\n",
       "      <td>21.537483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>15.288524</td>\n",
       "      <td>21.542604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>15.288360</td>\n",
       "      <td>21.432474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>15.288869</td>\n",
       "      <td>21.425890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>15.283325</td>\n",
       "      <td>21.390146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>15.283339</td>\n",
       "      <td>21.586784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>15.280916</td>\n",
       "      <td>21.411097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>15.279398</td>\n",
       "      <td>21.407286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>15.278278</td>\n",
       "      <td>21.479813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>15.277227</td>\n",
       "      <td>21.483145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>15.278706</td>\n",
       "      <td>21.553854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>15.278580</td>\n",
       "      <td>21.564146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>15.277768</td>\n",
       "      <td>21.470404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train/loss   val/loss\n",
       "0       0   16.388460  21.328363\n",
       "1       1   15.723532  21.301912\n",
       "2       2   15.568251  21.398142\n",
       "3       3   15.498148  21.538115\n",
       "4       4   15.455770  21.400196\n",
       "5       5   15.430855  21.358656\n",
       "6       6   15.408437  21.334433\n",
       "7       7   15.392024  21.515078\n",
       "8       8   15.381497  21.288002\n",
       "9       9   15.367785  21.428801\n",
       "10     10   15.357292  21.377481\n",
       "11     11   15.348079  21.396805\n",
       "12     12   15.343961  21.282213\n",
       "13     13   15.336177  21.400455\n",
       "14     14   15.332963  21.392611\n",
       "15     15   15.328608  21.469452\n",
       "16     16   15.325841  21.418718\n",
       "17     17   15.321985  21.345282\n",
       "18     18   15.321541  21.349298\n",
       "19     19   15.317352  21.325220\n",
       "20     20   15.315112  21.430372\n",
       "21     21   15.313273  21.325783\n",
       "22     22   15.311438  21.378805\n",
       "23     23   15.308382  21.441025\n",
       "24     24   15.307839  21.359484\n",
       "25     25   15.306304  21.492167\n",
       "26     26   15.307653  21.408672\n",
       "27     27   15.305036  21.348469\n",
       "28     28   15.305502  21.280390\n",
       "29     29   15.304422  21.386406\n",
       "30     30   15.304774  21.385309\n",
       "31     31   15.304351  21.343985\n",
       "32     32   15.302314  21.288002\n",
       "33     33   15.301056  21.275574\n",
       "34     34   15.300669  21.351576\n",
       "35     35   15.299047  21.414932\n",
       "36     36   15.298584  21.387619\n",
       "37     37   15.297479  21.325569\n",
       "38     38   15.293925  21.381674\n",
       "39     39   15.293931  21.451353\n",
       "40     40   15.292399  21.383957\n",
       "41     41   15.291520  21.537483\n",
       "42     42   15.288524  21.542604\n",
       "43     43   15.288360  21.432474\n",
       "44     44   15.288869  21.425890\n",
       "45     45   15.283325  21.390146\n",
       "46     46   15.283339  21.586784\n",
       "47     47   15.280916  21.411097\n",
       "48     48   15.279398  21.407286\n",
       "49     49   15.278278  21.479813\n",
       "50     50   15.277227  21.483145\n",
       "51     51   15.278706  21.553854\n",
       "52     52   15.278580  21.564146\n",
       "53     53   15.277768  21.470404"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NDCG@10_sampled</th>\n",
       "      <th>NDCG@10_unsampled</th>\n",
       "      <th>Recall@10_sampled</th>\n",
       "      <th>Recall@10_unsampled</th>\n",
       "      <th>Serendipity@10_sampled</th>\n",
       "      <th>Serendipity@10_unsampled</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>0.011827</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0</td>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1</td>\n",
       "      <td>4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2</td>\n",
       "      <td>7088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3</td>\n",
       "      <td>9451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4</td>\n",
       "      <td>11814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.011063</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.019054</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5</td>\n",
       "      <td>14177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.025624</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6</td>\n",
       "      <td>16540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.022996</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>18903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8</td>\n",
       "      <td>21266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.018152</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>9</td>\n",
       "      <td>23629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>10</td>\n",
       "      <td>25992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>11</td>\n",
       "      <td>28355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.025624</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>12</td>\n",
       "      <td>30718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.024940</td>\n",
       "      <td>0.023653</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>13</td>\n",
       "      <td>33081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.026308</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>14</td>\n",
       "      <td>35444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.028122</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>15</td>\n",
       "      <td>37807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>16</td>\n",
       "      <td>40170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.031410</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>17</td>\n",
       "      <td>42533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.033057</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>18</td>\n",
       "      <td>44896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.034712</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>19</td>\n",
       "      <td>47259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.036577</td>\n",
       "      <td>0.025624</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>20</td>\n",
       "      <td>49622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.038668</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>21</td>\n",
       "      <td>51985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.040436</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>22</td>\n",
       "      <td>54348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.042219</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>23</td>\n",
       "      <td>56711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.044053</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>24</td>\n",
       "      <td>59074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.045577</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>25</td>\n",
       "      <td>61437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.047374</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>26</td>\n",
       "      <td>63800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.048940</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>27</td>\n",
       "      <td>66163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.050608</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>28</td>\n",
       "      <td>68526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.002879</td>\n",
       "      <td>0.052337</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>29</td>\n",
       "      <td>70889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.054016</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>30</td>\n",
       "      <td>73252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.055822</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>31</td>\n",
       "      <td>75615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.057218</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>32</td>\n",
       "      <td>77978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.058730</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>33</td>\n",
       "      <td>80341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.060603</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>34</td>\n",
       "      <td>82704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.062059</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>35</td>\n",
       "      <td>85067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>36</td>\n",
       "      <td>87430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.065755</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>37</td>\n",
       "      <td>89793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.067372</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>38</td>\n",
       "      <td>92156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.069028</td>\n",
       "      <td>0.030880</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>39</td>\n",
       "      <td>94519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.070878</td>\n",
       "      <td>0.032852</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>40</td>\n",
       "      <td>96882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.072644</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>41</td>\n",
       "      <td>99245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.074362</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>42</td>\n",
       "      <td>101608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.076135</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>43</td>\n",
       "      <td>103971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.003379</td>\n",
       "      <td>0.077964</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>44</td>\n",
       "      <td>106334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.079585</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>45</td>\n",
       "      <td>108697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.081073</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>46</td>\n",
       "      <td>111060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.083246</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>47</td>\n",
       "      <td>113423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.084999</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>48</td>\n",
       "      <td>115786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>49</td>\n",
       "      <td>118149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.088224</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>50</td>\n",
       "      <td>120512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.090391</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>51</td>\n",
       "      <td>122875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.092088</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>52</td>\n",
       "      <td>125238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.093954</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>53</td>\n",
       "      <td>127601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NDCG@10_sampled  NDCG@10_unsampled  Recall@10_sampled  \\\n",
       "0          0.003752           0.001894           0.031537   \n",
       "1          0.003227           0.003946           0.026938   \n",
       "2          0.003435           0.005737           0.030223   \n",
       "3          0.002978           0.007539           0.026281   \n",
       "4          0.002953           0.009345           0.024310   \n",
       "5          0.002747           0.011063           0.024967   \n",
       "6          0.003015           0.013068           0.025624   \n",
       "7          0.002790           0.014837           0.022996   \n",
       "8          0.002847           0.016229           0.027595   \n",
       "9          0.002923           0.018152           0.024967   \n",
       "10         0.002714           0.019836           0.024310   \n",
       "11         0.002943           0.021578           0.026938   \n",
       "12         0.002883           0.023250           0.025624   \n",
       "13         0.002762           0.024940           0.023653   \n",
       "14         0.002911           0.026308           0.028252   \n",
       "15         0.002869           0.028122           0.024310   \n",
       "16         0.002950           0.029608           0.029566   \n",
       "17         0.003046           0.031410           0.026938   \n",
       "18         0.003038           0.033057           0.027595   \n",
       "19         0.003147           0.034712           0.028909   \n",
       "20         0.003139           0.036577           0.025624   \n",
       "21         0.003278           0.038668           0.026938   \n",
       "22         0.002968           0.040436           0.026281   \n",
       "23         0.003172           0.042219           0.024967   \n",
       "24         0.002951           0.044053           0.024967   \n",
       "25         0.002973           0.045577           0.027595   \n",
       "26         0.003126           0.047374           0.028909   \n",
       "27         0.003137           0.048940           0.028909   \n",
       "28         0.003125           0.050608           0.028252   \n",
       "29         0.002879           0.052337           0.024310   \n",
       "30         0.003035           0.054016           0.027595   \n",
       "31         0.003067           0.055822           0.026938   \n",
       "32         0.003213           0.057218           0.030223   \n",
       "33         0.002943           0.058730           0.028909   \n",
       "34         0.003375           0.060603           0.029566   \n",
       "35         0.002900           0.062059           0.029566   \n",
       "36         0.003159           0.063844           0.027595   \n",
       "37         0.003173           0.065755           0.028252   \n",
       "38         0.002826           0.067372           0.027595   \n",
       "39         0.003288           0.069028           0.030880   \n",
       "40         0.003405           0.070878           0.032852   \n",
       "41         0.003012           0.072644           0.026281   \n",
       "42         0.002953           0.074362           0.026938   \n",
       "43         0.002885           0.076135           0.024967   \n",
       "44         0.003379           0.077964           0.029566   \n",
       "45         0.003149           0.079585           0.030223   \n",
       "46         0.002834           0.081073           0.026281   \n",
       "47         0.003345           0.083246           0.028909   \n",
       "48         0.003406           0.084999           0.032194   \n",
       "49         0.002817           0.086451           0.026281   \n",
       "50         0.003270           0.088224           0.029566   \n",
       "51         0.003680           0.090391           0.031537   \n",
       "52         0.003071           0.092088           0.028909   \n",
       "53         0.003032           0.093954           0.026938   \n",
       "\n",
       "    Recall@10_unsampled  Serendipity@10_sampled  Serendipity@10_unsampled  \\\n",
       "0              0.011827                0.000011                  0.000002   \n",
       "1              0.014455                0.000073                  0.000002   \n",
       "2              0.015769                0.000076                  0.000002   \n",
       "3              0.017083                0.000008                  0.000003   \n",
       "4              0.017740                0.000007                  0.000003   \n",
       "5              0.019054                0.000007                  0.000003   \n",
       "6              0.019711                0.000008                  0.000003   \n",
       "7              0.019711                0.000007                  0.000003   \n",
       "8              0.019711                0.000008                  0.000003   \n",
       "9              0.019711                0.000007                  0.000003   \n",
       "10             0.019711                0.000006                  0.000003   \n",
       "11             0.019711                0.000008                  0.000003   \n",
       "12             0.019711                0.000008                  0.000003   \n",
       "13             0.019711                0.000007                  0.000003   \n",
       "14             0.019711                0.000009                  0.000003   \n",
       "15             0.019711                0.000007                  0.000003   \n",
       "16             0.019711                0.000009                  0.000003   \n",
       "17             0.020368                0.000008                  0.000003   \n",
       "18             0.020368                0.000009                  0.000003   \n",
       "19             0.020368                0.000010                  0.000003   \n",
       "20             0.020368                0.000007                  0.000003   \n",
       "21             0.020368                0.000008                  0.000003   \n",
       "22             0.020368                0.000008                  0.000003   \n",
       "23             0.020368                0.000007                  0.000003   \n",
       "24             0.020368                0.000007                  0.000003   \n",
       "25             0.020368                0.000008                  0.000003   \n",
       "26             0.021025                0.000009                  0.000003   \n",
       "27             0.021025                0.000008                  0.000003   \n",
       "28             0.021025                0.000008                  0.000003   \n",
       "29             0.021025                0.000007                  0.000003   \n",
       "30             0.021025                0.000008                  0.000003   \n",
       "31             0.021025                0.000008                  0.000003   \n",
       "32             0.021025                0.000009                  0.000003   \n",
       "33             0.021025                0.000008                  0.000003   \n",
       "34             0.021025                0.000008                  0.000003   \n",
       "35             0.021025                0.000009                  0.000003   \n",
       "36             0.021025                0.000008                  0.000003   \n",
       "37             0.021025                0.000008                  0.000003   \n",
       "38             0.021682                0.000008                  0.000003   \n",
       "39             0.021682                0.000010                  0.000003   \n",
       "40             0.021682                0.000010                  0.000003   \n",
       "41             0.021682                0.000008                  0.000003   \n",
       "42             0.021682                0.000008                  0.000003   \n",
       "43             0.021682                0.000008                  0.000003   \n",
       "44             0.021682                0.000009                  0.000003   \n",
       "45             0.021682                0.000009                  0.000003   \n",
       "46             0.021682                0.000008                  0.000003   \n",
       "47             0.021682                0.000009                  0.000003   \n",
       "48             0.021682                0.000010                  0.000003   \n",
       "49             0.021682                0.000008                  0.000003   \n",
       "50             0.021682                0.000009                  0.000003   \n",
       "51             0.021682                0.000010                  0.000003   \n",
       "52             0.021682                0.000009                  0.000003   \n",
       "53             0.021682                0.000008                  0.000003   \n",
       "\n",
       "    epoch    step  \n",
       "0       0    2362  \n",
       "1       1    4725  \n",
       "2       2    7088  \n",
       "3       3    9451  \n",
       "4       4   11814  \n",
       "5       5   14177  \n",
       "6       6   16540  \n",
       "7       7   18903  \n",
       "8       8   21266  \n",
       "9       9   23629  \n",
       "10     10   25992  \n",
       "11     11   28355  \n",
       "12     12   30718  \n",
       "13     13   33081  \n",
       "14     14   35444  \n",
       "15     15   37807  \n",
       "16     16   40170  \n",
       "17     17   42533  \n",
       "18     18   44896  \n",
       "19     19   47259  \n",
       "20     20   49622  \n",
       "21     21   51985  \n",
       "22     22   54348  \n",
       "23     23   56711  \n",
       "24     24   59074  \n",
       "25     25   61437  \n",
       "26     26   63800  \n",
       "27     27   66163  \n",
       "28     28   68526  \n",
       "29     29   70889  \n",
       "30     30   73252  \n",
       "31     31   75615  \n",
       "32     32   77978  \n",
       "33     33   80341  \n",
       "34     34   82704  \n",
       "35     35   85067  \n",
       "36     36   87430  \n",
       "37     37   89793  \n",
       "38     38   92156  \n",
       "39     39   94519  \n",
       "40     40   96882  \n",
       "41     41   99245  \n",
       "42     42  101608  \n",
       "43     43  103971  \n",
       "44     44  106334  \n",
       "45     45  108697  \n",
       "46     46  111060  \n",
       "47     47  113423  \n",
       "48     48  115786  \n",
       "49     49  118149  \n",
       "50     50  120512  \n",
       "51     51  122875  \n",
       "52     52  125238  \n",
       "53     53  127601  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
